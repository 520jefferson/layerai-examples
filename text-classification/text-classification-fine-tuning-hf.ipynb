{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open in Layer](https://development.layer.co/assets/badge.svg)](https://development.layer.co/layer/derrick-bert) [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/layerai/examples/blob/main/text-classification/text-classification-fine-tuning-hf.ipynb) [![Layer Examples Github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/layerai/examples/tree/main/text-classification)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine tuning Hugging Face BERT model for text classification\n",
    "In this notebook we fine tune a [BERT model](https://huggingface.co/bert-base-uncased) for text classification."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets -qqq\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ],
   "metadata": {
    "id": "pLeCYHBfKD-v",
    "outputId": "f2f9697f-ea39-4265-f448-48730e984e65",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install layer-sdk --upgrade -qqq"
   ],
   "metadata": {
    "id": "sHKwEybKVH1t",
    "outputId": "d74ffcfd-548b-4954-9dce-8fbd5bfb3bd1",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import layer\n",
    "from layer.decorators import model,pip_requirements,fabric\n",
    "layer.login(\"https://development.layer.co\")\n",
    "layer.init(\"derrick-bert\")"
   ],
   "metadata": {
    "id": "WhM6iGPSVKw4",
    "outputId": "51fa6faa-2c3a-42bc-e53c-ea5a1a1dec9a",
    "execution": {
     "iopub.status.busy": "2022-04-04T13:54:55.202510Z",
     "iopub.execute_input": "2022-04-04T13:54:55.202802Z",
     "iopub.status.idle": "2022-04-04T13:54:56.177464Z",
     "shell.execute_reply.started": "2022-04-04T13:54:55.202770Z",
     "shell.execute_reply": "2022-04-04T13:54:56.176736Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "@pip_requirements(packages=[\"transformers\",\"sentencepiece\"])\n@fabric(\"f-medium\")\n@model(name=\"bert-tokenizer\")\ndef download_tokenizer():\n    from transformers import BertTokenizer\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n    return tokenizer",
   "metadata": {
    "id": "tdd77juPVvMu",
    "execution": {
     "iopub.status.busy": "2022-04-04T14:56:06.535672Z",
     "iopub.execute_input": "2022-04-04T14:56:06.535937Z",
     "iopub.status.idle": "2022-04-04T14:56:06.541301Z",
     "shell.execute_reply.started": "2022-04-04T14:56:06.535905Z",
     "shell.execute_reply": "2022-04-04T14:56:06.540249Z"
    },
    "trusted": true
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Run the project on Layer Infra\n# layer.run([download_tokenizer])\ndownload_tokenizer()",
   "metadata": {
    "id": "vATfdH_bVvz4",
    "outputId": "2f76e6a4-1701-4017-bc36-8d590c8accfc",
    "execution": {
     "iopub.status.busy": "2022-04-04T14:56:08.661514Z",
     "iopub.execute_input": "2022-04-04T14:56:08.661772Z",
     "iopub.status.idle": "2022-04-04T14:56:14.310087Z",
     "shell.execute_reply.started": "2022-04-04T14:56:08.661742Z",
     "shell.execute_reply": "2022-04-04T14:56:14.309230Z"
    },
    "trusted": true
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def tokenize_function(examples):\n    from transformers import BertTokenizer\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)",
   "metadata": {
    "id": "u76adh7LYbwa",
    "execution": {
     "iopub.status.busy": "2022-04-04T14:57:12.245311Z",
     "iopub.execute_input": "2022-04-04T14:57:12.246125Z",
     "iopub.status.idle": "2022-04-04T14:57:12.251628Z",
     "shell.execute_reply.started": "2022-04-04T14:57:12.246073Z",
     "shell.execute_reply": "2022-04-04T14:57:12.250331Z"
    },
    "trusted": true
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "@pip_requirements(packages=[\"transformers\",\"sentencepiece\"])\n@fabric(\"f-medium\")\n@model(\"bert\")\ndef train():    \n    import tensorflow as tf\n    from transformers import TFAutoModelForSequenceClassification\n    from transformers import DefaultDataCollator\n    from transformers import AutoTokenizer\n    from datasets import load_dataset\n\n    dataset = load_dataset(\"imdb\")\n\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n    small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n    small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\n    data_collator = DefaultDataCollator(return_tensors=\"tf\")\n\n    tf_train_dataset = small_train_dataset.to_tf_dataset(\n        columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n        label_cols=[\"labels\"],\n        shuffle=True,\n        collate_fn=data_collator,\n        batch_size=8,)\n\n    tf_validation_dataset = small_eval_dataset.to_tf_dataset(\n        columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n        label_cols=[\"labels\"],\n        shuffle=False,\n        collate_fn=data_collator,\n        batch_size=8,)\n    model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=tf.metrics.SparseCategoricalAccuracy(),)\n    model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)\n    return model",
   "metadata": {
    "id": "OpSK2gc3KD-_",
    "execution": {
     "iopub.status.busy": "2022-04-04T14:57:12.869747Z",
     "iopub.execute_input": "2022-04-04T14:57:12.870497Z",
     "iopub.status.idle": "2022-04-04T14:57:12.881735Z",
     "shell.execute_reply.started": "2022-04-04T14:57:12.870455Z",
     "shell.execute_reply": "2022-04-04T14:57:12.881031Z"
    },
    "trusted": true
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# layer.run([train])\ntrain()",
   "metadata": {
    "id": "DgRB_oc9S7Hx",
    "outputId": "062d7722-3cca-438a-a8ac-6d465ba02345",
    "execution": {
     "iopub.status.busy": "2022-04-04T14:57:13.402711Z",
     "iopub.execute_input": "2022-04-04T14:57:13.402957Z",
     "iopub.status.idle": "2022-04-04T15:20:23.333912Z",
     "shell.execute_reply.started": "2022-04-04T14:57:13.402927Z",
     "shell.execute_reply": "2022-04-04T15:20:23.332801Z"
    },
    "trusted": true
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "bert = layer.get_model('bert').get_train()\nbert",
   "metadata": {
    "id": "BDc-mNQxVmzx",
    "execution": {
     "iopub.status.busy": "2022-04-04T15:20:23.342024Z",
     "iopub.execute_input": "2022-04-04T15:20:23.345747Z",
     "iopub.status.idle": "2022-04-04T15:20:29.840623Z",
     "shell.execute_reply.started": "2022-04-04T15:20:23.345709Z",
     "shell.execute_reply": "2022-04-04T15:20:29.839907Z"
    },
    "trusted": true
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "tokenizer = layer.get_model('bert-tokenizer').get_train()\ninput_sequence = \"I really loved that movie, the script was on point\"\n# encode context the generation is conditioned on\ninput_ids = tokenizer.encode(input_sequence, return_tensors='tf')\noutput = bert(input_ids)\nlogits = output.logits",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-04T15:20:29.842557Z",
     "iopub.execute_input": "2022-04-04T15:20:29.843113Z",
     "iopub.status.idle": "2022-04-04T15:20:32.826796Z",
     "shell.execute_reply.started": "2022-04-04T15:20:29.843067Z",
     "shell.execute_reply": "2022-04-04T15:20:32.826089Z"
    },
    "trusted": true
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "logits",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-04T15:20:32.828225Z",
     "iopub.execute_input": "2022-04-04T15:20:32.828491Z",
     "iopub.status.idle": "2022-04-04T15:20:32.834453Z",
     "shell.execute_reply.started": "2022-04-04T15:20:32.828453Z",
     "shell.execute_reply": "2022-04-04T15:20:32.833778Z"
    },
    "trusted": true
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# https://huggingface.co/docs/transformers/main/en/model_doc/distilbert\nimport tensorflow as tf\npredicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\nbert.config.id2label[predicted_class_id]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-04T15:20:32.835972Z",
     "iopub.execute_input": "2022-04-04T15:20:32.836484Z",
     "iopub.status.idle": "2022-04-04T15:20:32.846689Z",
     "shell.execute_reply.started": "2022-04-04T15:20:32.836399Z",
     "shell.execute_reply": "2022-04-04T15:20:32.845832Z"
    },
    "trusted": true
   },
   "execution_count": 47,
   "outputs": []
  }
 ]
}