{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Layer best practices.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Layer best practices\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/layerai/examples/blob/main/tutorials/best-practices/best_practices.ipynb) [![Layer Examples Github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/layerai/examples/tree/main/tutorials/best-practices)\n",
        "\n",
        "In this article, we will look at how get the most out of [Layer](www.layer.ai)."
      ],
      "metadata": {
        "id": "lFFa_cJy7GAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset saving"
      ],
      "metadata": {
        "id": "PcOTWZUn8f95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often, you need to process some data and save it so as not to repeat the preprocessing steps. When working with datasets in Layer, we recommend that you: \n",
        "- Import the packages needed to preprocess the data in the dataset function. \n",
        "- Download or load the dataset in the dataset function.\n",
        "- Upload your datasets to Layer, it will make using them in subsequent runs faster.  \n",
        "\n",
        "\n",
        "\n",
        "Downloading the dataset outside the dataset function means that this data will be uploaded to Layer when you run the function. Writing the download code in the dataset function ensures that the data is downloaded directly in the container where the function is running on Layer infra. This will save you a lot of time especially when dealing with large dataset downloads. \n",
        "\n",
        "Expensive preprocessing steps should also be written inside the dataset function for the same reason. \n",
        "\n",
        "For example, the code below can be refactored with the above information in mind. \n",
        "\n",
        "`pip install layer -qqq`\n",
        "\n",
        "```\n",
        "import wget \n",
        "import pandas as pd\n",
        "wget.download(url)\n",
        "pd.read_csv(large_downloaded_data)\n",
        "\n",
        "```\n",
        "\n",
        "Reading this data as a [Pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) inside the dataset function means that the data will have to be uploaded to Layer. The alternative is to use Layer resources to download this dataset. Here's how this would look like in practice: \n",
        "```\n",
        "import layer \n",
        "from layer.decorators import dataset, pip_requirements\n",
        "layer.login()\n",
        "layer.init(\"project_name\")\n",
        "@dataset(\"dataset_name\")\n",
        "@pip_requirements(packages=[\"wget\"])\n",
        "def save_data():\n",
        "  import wget \n",
        "  import pandas as pd\n",
        "  wget.download(url)\n",
        "  df = pd.read_csv(large_downloaded_data)\n",
        "  return df\n",
        "layer.run([save_data])\n",
        "\n",
        "```\n",
        "Passing the `save_data` function to Layer means that all the instructions inside this function will be executed on Layer infra. However, if you have downloaded any large files outside this function, Layer will first pickle them and upload them. You can save some precious time by writing the download instuctions in the dataset function so that the download happens directly on Layer infra. "
      ],
      "metadata": {
        "id": "ACkGYprEH4_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "PACXP8KI8Mfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In some cases, you will define large models or need to use large pre-trained models. We recommend that you write the model defiition inside the `train` function. The reasoning similar to the one we just mentioned in the dataset section above. For example, when building deep learning models we recommend that you write the instructions to download images in the training fuction. Doing otherwise means that you will have to endure longer waiting time as the images are uploaded. Writing the download instructions in the train function downloads the images on the Layer infra and they are ready to use immediately. Here is a code snippet showing how you might download some images and extract them on Layer infra.\n",
        "```\n",
        "@pip_requirements(packages=[\"wget\"])\n",
        "@fabric(\"f-gpu-small\")\n",
        "@model(name=\"food-vision\")\n",
        "def train():\n",
        "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import Sequential\n",
        "    from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Dropout\n",
        "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "    from tensorflow.keras.callbacks import EarlyStopping\n",
        "    import os\n",
        "    import matplotlib.pyplot as plt\n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import tarfile\n",
        "    import wget\n",
        "    wget.download(url)\n",
        "    food_tar = tarfile.open('data.tar.gz')\n",
        "    food_tar.extractall('.')\n",
        "    food_tar.close()\n",
        "\n",
        "    base_dir = '...'\n",
        "    class_names = os.listdir(base_dir)\n",
        "    train_datagen = ImageDataGenerator(...)\n",
        "    validation_gen = ImageDataGenerator(rescale=1./255,validation_split=0.2)\n",
        "    image_size = (200, 200)\n",
        "    training_set = train_datagen.flow_from_directory(...)\n",
        "    validation_set = validation_gen.flow_from_directory(...)\n",
        "    model =......\n",
        "    model.compile()\n",
        "    epochs=10\n",
        "    history = model.fit(..)\n",
        "    metrics_df = pd.DataFrame(history.history)\n",
        "    layer.log({\"Metrics\":metrics_df})\n",
        "    loss, accuracy = model.evaluate(..)\n",
        "    layer.log({\"Accuracy on test dataset\":accuracy})\n",
        "    metrics_df[[\"loss\",\"val_loss\"]].plot()\n",
        "    layer.log({\"Loss plot\":plt.gcf()})\n",
        "    metrics_df[[\"categorical_accuracy\",\"val_categorical_accuracy\"]].plot()\n",
        "    layer.log({\"Accuracy plot\":plt.gcf()})\n",
        "    return model\n",
        "  layer.run([train])\n",
        "\n",
        "```"
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declare dependancies \n"
      ],
      "metadata": {
        "id": "wW4MZQ8mInO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is good practice to declare dependencies when building entites that depend on other Layer entities. This enables Layer to optimize your pipeline. You can declare dependencies for models and datasets. \n",
        "\n",
        "\n",
        "```\n",
        "from layer import Dataset, Model\n",
        "\n",
        "#MODEL DECORATOR WITH DEPENDENCIES\n",
        "@model(\"clustering_model\",dependencies=[Dataset(\"product_ids_and_vectors\")])\n",
        "\n",
        "#DATASET DECORATOR WITH DEPENDENCIES\n",
        "@dataset(\"final_product_clusters\", dependencies=[Model(\"clustering_model\"), Dataset(\"product_ids_and_vectors\")])\n",
        "```"
      ],
      "metadata": {
        "id": "DTwmHcilSNOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pip requirements\n",
        "[Layer fabrics](https://docs.layer.ai/docs/reference/fabrics) are pre-installed with common data science packages to make your development work faster. Check the versions of these [packages](https://docs.layer.ai/docs/reference/fabrics#preinstalled-libraries) to make sure that your project uses those versions. However, if the package versions are different, we recommend that you declare the exact version to prevent any errors. This can be done using the [pip_requirements decorator](https://docs.layer.ai/docs/sdk-library/pip-requirements-decorator) as shown below:\n",
        "\n",
        "```python\n",
        "@pip_requirements(packages=[\"pandas==1.3.5\",\"Keras==2.6.0\",\"scikit-learn==1.0.2\"])\n",
        "@model(name=\"model_name\")\n",
        "def train():\n",
        "    pass\n",
        "```\n",
        "\n",
        "This can be done for datasets as well:\n",
        "```\n",
        "@pip_requirements(packages=[\"pandas==1.3.5\",\"Keras==2.6.0\",\"scikit-learn==1.0.2\"])\n",
        "@dataset(name=\"dataset_name\")\n",
        "def save_data():\n",
        "    pass\n",
        "```"
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Where to go from here\n",
        "To learn more about using layer, you can: \n",
        "- Join our [Slack Community ](https://bit.ly/layercommunityslack)\n",
        "- Visit [Layer Examples Repo](https://github.com/layerai/examples) for more examples\n",
        "- Browse [Trending Layer Projects](https://layer.ai) on our mainpage\n",
        "- Check out [Layer Documentation](https://docs.layer.ai) to learn more"
      ],
      "metadata": {
        "id": "lwBbwXxYXLwW"
      }
    }
  ]
}
