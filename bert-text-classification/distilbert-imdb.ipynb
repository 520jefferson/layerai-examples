{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2640f7a6",
   "metadata": {},
   "source": [
    "# Sentiment Classification of IMDB Reviews Using DistilBERT\n",
    "\n",
    "\n",
    "[![Open in Layer](https://development.layer.co/assets/badge.svg)](https://app.layer.ai/douglas_mcilwraith/bert-text-classification/) [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/layerai/examples/blob/main/bert-text-classification/bert-text-classification.ipynb) [![Layer Examples Github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/layerai/examples/tree/main/bert-text-classification)\n",
    "\n",
    "We use the DistilBERT [] to perform sentiment classification on the [IMDB sentiment dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install layer -U\n",
    "\n",
    "import layer\n",
    "layer.login()\n",
    "\n",
    "from layer.decorators import model, dataset, fabric, pip_requirements\n",
    "layer.init(\"distilbert-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8170e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess our training dataset and upload to layer\n",
    "@dataset(\"imdb-train\")\n",
    "@pip_requirements(packages=[\"datasets\"])\n",
    "def build():\n",
    "    from datasets import load_dataset\n",
    "    import pandas as pd\n",
    "\n",
    "    ds = load_dataset(\"imdb\")['train']\n",
    "    df = pd.DataFrame(ds)\n",
    "    return df\n",
    "\n",
    "layer.run([build])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f50552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess our test dataset and upload to layer\n",
    "@dataset(\"imdb-test\")\n",
    "@pip_requirements(packages=[\"datasets\"])\n",
    "def build():\n",
    "    from datasets import load_dataset\n",
    "    import pandas as pd\n",
    "\n",
    "    ds = load_dataset(\"imdb\")['test']\n",
    "    df = pd.DataFrame(ds)\n",
    "    return df\n",
    "\n",
    "layer.run([build])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tune the DistilBERT model using 10% of the train data (randomly sampled)\n",
    "@fabric(\"f-gpu-small\")\n",
    "@model('bert-fine-tune')\n",
    "def train():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from transformers import TrainingArguments, Trainer\n",
    "    from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "    from transformers import EarlyStoppingCallback\n",
    "    \n",
    "\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    data = layer.get_dataset(\"douglas_mcilwraith/distilbert-imdb/datasets/imdb-train:1.1\").to_pandas()\n",
    "    data = data.sample(frac=0.10, replace=False, random_state=2)\n",
    "\n",
    "    X = list(data[\"text\"])\n",
    "    y = list(data[\"label\"])\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "    X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    class MyDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels=None):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            if self.labels:\n",
    "                item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.encodings[\"input_ids\"])\n",
    "    \n",
    "    train_dataset = MyDataset(X_train_tokenized, y_train)\n",
    "    val_dataset = MyDataset(X_val_tokenized, y_val)\n",
    "    \n",
    "    \n",
    "    def calc_metrics(p):\n",
    "        pred = p[0]\n",
    "        labels = p[1]\n",
    "        pred = np.argmax(pred, axis=1)\n",
    "        \n",
    "        a = accuracy_score(y_pred=pred, y_true=labels)\n",
    "        r = recall_score(y_pred=pred, y_true=labels)\n",
    "        p = precision_score(y_pred=pred, y_true=labels,)\n",
    "        f = f1_score(y_pred=pred, y_true=labels)\n",
    "        \n",
    "        metrics = {\"accuracy\": a,\"precision\": p, \"recall\": r, \"f1\": f}\n",
    "        \n",
    "        #log the metrics from the latest evaluation to the UI\n",
    "        layer.log(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"out\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=50,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=calc_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    return model\n",
    "\n",
    "layer.run([train],debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate every 10% of the test data against the fine tuned model.\n",
    "@fabric(\"f-gpu-small\")\n",
    "@model(\"distilbert-evaluation\")\n",
    "def build():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import torch\n",
    "    from transformers import TrainingArguments, Trainer\n",
    "    from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "    from transformers import EarlyStoppingCallback\n",
    "\n",
    "    #We need to use the same tokenizer as we did during training\n",
    "    \n",
    "    #We need to use the same tokenizer as we did during training\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    df = layer.get_dataset(\"douglas_mcilwraith/distilbert-imdb/datasets/imdb-test:1.1\").to_pandas()\n",
    "    df = df.sample(frac=1)\n",
    "    \n",
    "    my_model = layer.get_model(\"douglas_mcilwraith/distilbert-imdb/models/bert-fine-tune:5.1\").get_train()\n",
    "    trainer = Trainer(my_model)\n",
    "    \n",
    "    s = np.array_split(df,5)\n",
    "    list_results = []\n",
    "\n",
    "    for d in s:\n",
    "        X = list(d[\"text\"])\n",
    "        y = list(d[\"label\"])\n",
    "\n",
    "        df_tokenized = tokenizer(X, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        class MyDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self, encodings, labels=None):\n",
    "                self.encodings = encodings\n",
    "                self.labels = labels\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "                if self.labels:\n",
    "                    item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "                return item\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "        test_dataset = MyDataset(df_tokenized, y)\n",
    "        p, _, _ = trainer.predict(test_dataset)\n",
    "\n",
    "        pred = np.argmax(p, axis=1)\n",
    "        labels = test_dataset.labels\n",
    "\n",
    "        a = accuracy_score(y_pred=pred, y_true=labels)\n",
    "        r = recall_score(y_pred=pred, y_true=labels)\n",
    "        p = precision_score(y_pred=pred, y_true=labels,)\n",
    "        f = f1_score(y_pred=pred, y_true=labels)\n",
    "\n",
    "        results = [a, r, p, f]\n",
    "        list_results.append(results)\n",
    "\n",
    "    results_df = pd.DataFrame(data=list_results, columns=['Accuracy', 'Precision', 'Recall',\"F1\"])\n",
    "    layer.log({\"results\" : results_df})\n",
    "    \n",
    "    return my_model\n",
    "\n",
    "layer.run([build],debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
