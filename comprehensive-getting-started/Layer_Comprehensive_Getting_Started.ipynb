{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqEOyv_g6vbB"
      },
      "source": [
        "# EXPERIMENT TRACKING FOR ML PROJECTS\n",
        "\n",
        "In this tutorial, we will discuss what experiment tracking means within the Machine Learning context, why it is important to keep track of your project journey and how you could make use of [Layer](https://layer.ai/) to do it.<br><br>\n",
        "ML project development is experimental by its nature. It means that we need to run many recurrent experiments to end up with the best performing model. These experiments might include various tasks such as parameter searching to optimise a model or transforming a dataset in an incremental manner. You could think of these experiments as your Python functions in your notebook designed for conducting a specific task.<br><br>\n",
        "\n",
        "***[Experiment Tracking](https://layer.ai/):***<br> \n",
        "*\\\"During development of an ML project, being able to save entire project history and thus revert to any previous project snapshot when needed.\\\"*<br><br>\n",
        "Let's list possible actions data scientists should take during an ML project development and experiment tracking:<br><br>\n",
        "- **Versioning:** As GitHub is doing for code, datasets and models should be versioned automatically each time you build them so that whenever needed entities could be reverted to any previous version. It is sort of applying the CI/CD software principle for data projects as well.<br> \n",
        "- **Monitoring:** Display some information about datasets and models such as model performance metrics, model parameters or data profiling. It will provide continuous observability on entities and comparing different versions. \n",
        "Testing: Sanity check on datasets and models by running simple unit tests to have more reliable pipeline. It could be perceived as an Alert System for data projects.<br>\n",
        "- **Documentation:** Writing descriptions about datasets and models as well as a detailed project report. It will tell other people what a project is all about and break work in-silos.<br>\n",
        "- **Using more computing power:** Processing big data and training complicated algorithms generally require more resources such as CPU/GPU  utilisation.<br> \n",
        "- **Collecting everything in a single place:** Keep everything - datasets, models, project documentations - in a single place including list of all experiments (runs) conducted by all team members. This will enable collaborative working within the team.\n",
        "\n",
        "Now, let me walk you through a real but simple ML project development cycle and show how you could do all the things we listed above with the help of [Layer](https://layer.ai/).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUWbLf-99xW1"
      },
      "source": [
        "\n",
        "# CASE STUDY: COMREHENSIVE GETTING STARTED DEMO WITH [LAYER](https://layer.ai/)\n",
        "\n",
        "This is a step-by-step tutorial aims walking you through a simple yet comprehensive demo to get started with [Layer](https://layer.ai/).\n",
        "\n",
        "- **Step I:** Install [Layer](https://layer.ai/) and Authentication\n",
        "- **Step II:** Initialise a project and import some [Layer](https://layer.ai/) modules\n",
        "- **Step III:** Learning more about Dataset Versioning on [Layer](https://layer.ai/)\n",
        "- **Step IV:** Learning more about Model Versioning on [Layer](https://layer.ai/)\n",
        "- **Step V:** Run multiple-functions experiments on [Layer](https://layer.ai/)\n",
        "- **Step VI:** Upload and Create Dynamic Project Documentation\n",
        "- **Step VII:** Run [Layer](https://layer.ai/) in local model\n",
        "<br><br>\n",
        "\n",
        "You will learn more about these decorators and functions in this exact order:\n",
        "- layer.init() function\n",
        "- @dataset decorator\n",
        "- @assert_not_null decorator\n",
        "- @resources decorator\n",
        "- layer.log() function\n",
        "- layer.run() function\n",
        "- @model decorator\n",
        "- @fabric decorator\n",
        "- @assert_true decorator\n",
        "- get_dataset() function\n",
        "- get_model() function\n",
        "<br><br>\n",
        "\n",
        "You are also about to learn more about these important concepts and features of [Layer](https://layer.ai/):\n",
        "- How Dataset Versioning works on [Layer](https://layer.ai/)\n",
        "- How Model Versioning works on [Layer](https://layer.ai/)\n",
        "- How to compare different model versions\n",
        "- How to share and import a Dataset or Model\n",
        "- How to run and manage multiple experiments on [Layer](https://layer.ai/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rY4aiBP_gCy"
      },
      "source": [
        "# **Step I:** Install Layer and Authentication\n",
        "---\n",
        "\n",
        "With just 3 lines of code, you will be done with installation and authentication in your notebook.<br><br>\n",
        "Once you run the code cell below, it will prompt you to a link like below.\n",
        "\n",
        "![img](https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-07%20at%2018.20.58.png?raw=true)\n",
        "\n",
        "---\n",
        "\n",
        "If you are a first time user on [Layer](https://layer.ai/), then you will see these 5 authentication pages in the exact order below when you open the link in the screenshot above.\n",
        "\n",
        "\n",
        "<p float=\"left\">\n",
        "  <img src=\"https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-07%20at%2018.21.19.png?raw=true\" width=\"300\" />\n",
        "  <img src=\"https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-07%20at%2018.21.40.png?raw=true\" width=\"300\" />\n",
        "  <img src=\"  https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-07%20at%2018.21.52.png?raw=true\" width=\"300\" /> \n",
        "  <img src=\"  https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-07%20at%2018.22.17.png?raw=true\" width=\"500\" height=\"300\" /> \n",
        "  <img src=\"  https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-07%20at%2018.22.57.png?raw=true\" width=\"500\" height=\"300\" />  \n",
        "</p>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone project github repo into your Colab environment\n",
        "!rm -rf examples\n",
        "!git clone https://github.com/layerai/examples/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOKbz9uuakNQ"
      },
      "outputs": [],
      "source": [
        "# Step I: Install Layer and Authentication\n",
        "\n",
        "# Run the shell command to install Layer Python package\n",
        "!pip install layer -U\n",
        "# Import Layer package\n",
        "import layer\n",
        "# Call Layer's login function to authenticate\n",
        "layer.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Qm4gGMtJv5"
      },
      "source": [
        "\n",
        "# **Step II:** Initialise a project and import some Layer modules\n",
        "---\n",
        "\n",
        "Once you paste the authentication code into the Python prompt on your notebook, you will successfully log into [Layer](https://layer.ai/)! Click on the link https://app.layer.ai and you will see the landing page of [Layer](https://layer.ai/).\n",
        "\n",
        "  <img src=\"  https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-07%20at%2018.56.52.png?raw=true\" width=\"700\" height=\"400\" />\n",
        "\n",
        "Click on **My Projects** tab on the top bar. Once you create several projects using [Layer](https://layer.ai/), they will all list here. Click on the **New project** button and you will see a pop-up page to enter some details of your project.\n",
        "\n",
        "  <img src=\"  https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-07%20at%2019.08.51.png?raw=true\" width=\"700\" height=\"400\" />\n",
        "\n",
        "  Once you create your project, you should see the empty project page of your project named **my-first-project** as seen in the image below.\n",
        "\n",
        "  <img src=\"  https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-14%20at%2010.59.27.png?raw=true::\" width=\"700\" height=\"400\" />\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQT4pThP1j5U"
      },
      "source": [
        "\n",
        "### Layer Decorators or Functions Used In Step-II\n",
        "---\n",
        "\n",
        "**layer.init():**<br>\n",
        "To start logging into the ***my-first-project*** project, [Layer](https://layer.ai/)'s init function should be called with the name of project as shown in the first line below. It basically tells [Layer](https://layer.ai/) that everything will be saved in that specific project. If any project exists with the given name, then it will match and sync otherwise, a new project with the given name will be created under your organisation or personal account. This function also has a ***pip_packages*** parameter which takes a list of Python packages. Some of the libraries are already pre-installed on Layer's remote machines. Check them out here: https://docs.app.layer.ai/docs/reference/fabrics#preinstalled-libraries You are expected to list all other libraries or different versions of the existing libraries you use in your project using the ***pip_packages*** argument. [Layer](https://layer.ai/) will install those Python packages first to make your project environment ready on the [Layer](https://layer.ai/) Infra. <br><br>\n",
        "\n",
        "For more information about this function:<br>\n",
        "https://docs.app.layer.ai/docs/sdk-library/layer-init\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6G61hqJlf19"
      },
      "outputs": [],
      "source": [
        "# Step II: Project initialization and import some Layer packages\n",
        "\n",
        "# Matches and Sync with the project we created before and installs the required Python packages on the Layer Remote\n",
        "layer.init(\"my-first-project\",pip_packages=['matplotlib','sklearn'])\n",
        "# Import functions from Layer\n",
        "from layer.decorators import dataset, model, resources, fabric\n",
        "from layer.decorators.assertions import assert_not_null, assert_true\n",
        "from layer import Dataset,Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfBOajgPMZ0H"
      },
      "source": [
        "# **Step III:** Learning more about Dataset Versioning on Layer\n",
        "---\n",
        "Every time you run your data build function, [Layer](https://layer.ai/) will automatically generate a new version of your dataset depending on changes in your function.<br><br>\n",
        "This is how [Layer](https://layer.ai/) does dataset versioning:<br><br>\n",
        "**- If schema of a dataset is changed, then bump up the major version number**<br>\n",
        "*(e.g v1.3 → v2.1)* <br>\n",
        "**- Otherwise, bump up the minor version number <br>**\n",
        "*(e.g v1.3 → v1.4)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBGcbBTPOn-a"
      },
      "source": [
        "\n",
        "\n",
        "### Layer Decorators and Functions Used in Step-III\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**@dataset:**<br> \n",
        "It is a decorator which defines the Pandas data frame ***training_df*** returned by the function as a [Layer](https://layer.ai/) dataset named ***my_first_dataset*** under the project ***my_first_project***. [Layer](https://layer.ai/) starts tracking this dataset automatically every time you run this function. In other words, [Layer](https://layer.ai/) will do versioning on your dataset and log any other data you want [Layer](https://layer.ai/) to store along with this dataset. It is important to note that it is required that you use this dataset decorator at the top a Python function returning a Pandas data frame.<br><br>\n",
        "For more information about the dataset decorator:<br>\n",
        "https://docs.app.layer.ai/docs/sdk-library/dataset-decorator <br><br>\n",
        "\n",
        "**@assert_not_null:**<br>\n",
        "It is one of pre-defined assertion decorators on [Layer](https://layer.ai/) which tests whether specified columns listed in its parameter have any null values or not. You could think of this as defining unit tests for your ML metadata. It alerts you right away if tests have passed or failed. Assertions ensure that nothing unexpected could happen in your dataset without your supervision.<br><br> \n",
        "For more information about the assertions: <br>\n",
        "https://docs.app.layer.ai/docs/sdk-library/assertions<br><br>\n",
        "\n",
        "**@resources:**<br> \n",
        "It's a decorator that is used to upload local files to [Layer](https://layer.ai/) remote machines. In this example, it is used to upload files under the ***data*** directory on Google Colab file system to the [Layer](https://layer.ai/) Cloud. Folder structure will be exactly the same on the cloud side as your local so that you don't have to change any hard-coded file paths exist in your code in order to run your function remotely.<br><br> \n",
        "For more information about the resources decorator:<br>\n",
        "https://docs.app.layer.ai/docs/sdk-library/resources-decorator<br><br>\n",
        "\n",
        "\n",
        "**layer.log():**<br>\n",
        "In the function body, there is also one more [Layer](https://layer.ai/) supported function which is used to log arbitrary data along with the dataset. This function supports many different data types such as primitives, dataframes or images. In this example, we log a box plot of the column named ***target***. In general, you could log anything attached with the current build version of your dataset or model by using this function.\n",
        "<br><br> \n",
        "For more information about the resources decorator:<br>\n",
        "https://docs.app.layer.ai/docs/sdk-library/layer-log<br><br>\n",
        "\n",
        "**layer.run():**<br>\n",
        "Runs specified list of functions in its parameter on [Layer](https://layer.ai/) infrastructure remotely. In the case below, since we have a function named ***create_dataset***, we just put the function name into a list and give it to this function.\n",
        "\n",
        "Check out the link to learn more:<br>\n",
        "https://docs.app.layer.ai/docs/sdk-library/layer-run\n",
        " \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8vKAtvLzZH7"
      },
      "outputs": [],
      "source": [
        "# --- Create your first dataset ---\n",
        "@dataset(\"my_first_dataset\")\n",
        "@assert_not_null([\"target\",\"feat1\"])\n",
        "@resources(path=\"./examples/comprehensive-getting-started/data\")\n",
        "def create_dataset():\n",
        "  from matplotlib import pyplot as plt\n",
        "  from sklearn.datasets import make_regression\n",
        "  import pandas as pd\n",
        "  \n",
        "  X_train, Y_train = make_regression(n_samples=100, n_features=3, n_targets=1, noise=0.5)\n",
        "\n",
        "  features = pd.DataFrame(X_train, columns = ['feat1', 'feat2', 'feat3'])\n",
        "  target = pd.DataFrame(Y_train, columns = ['target'])\n",
        "  training_df= pd.concat([features, target], axis=1)\n",
        "\n",
        "  external_df = pd.read_csv(\"examples/comprehensive-getting-started/data/external.csv\",names=['feat1','feat2','feat3','target'],header=None)\n",
        "  final_training_df = training_df.append(external_df)\n",
        "\n",
        "  training_df[['target']].plot(kind='box', title='Target Quartile Analysis')\n",
        "  layer.log({\"Box plots\": plt})\n",
        "\n",
        "  plt.close()\n",
        "  \n",
        "  return training_df "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnaPlmrP4Yg3"
      },
      "source": [
        "Now, we are ready to take one more step to run the function above on [Layer](https://layer.ai/) Cloud. <br>\n",
        "\n",
        "When you run the cell below for the first time, it will create your dataset ***my_first_dataset v1.1*** and also print a link to that dataset's page on [Layer](https://layer.ai/) Web UI. Once you click on the generated link, you will see a dataset page looks like the one below:\n",
        "\n",
        "  <img src=\" https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-14%20at%2012.26.03.png?raw=true\" width=\"700\" height=\"500\" />\n",
        "\n",
        "Once you run the cell below for the second time without changing anything in the ***create_dataset*** function's code, [Layer](https://layer.ai/) will generate a new version of the same dataset and bump up only the minor version number *(v1.1 --> v1.2)* since there will be no change in schema of the dataset. When you click on newly generated link, you will be directed to the ***my_first_dataset v1.2***'s page.\n",
        "\n",
        "  <img src=\" https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-14%20at%2012.28.22.png?raw=true\" width=\"700\" height=\"500\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crHeFPUVLryD"
      },
      "outputs": [],
      "source": [
        "# --- Run your function: create_dataset remotely on Layer Infra ---\n",
        "\n",
        "# Run this cell twice to create my_first_dataset v1.1 & v1.2 \n",
        "# and check them out on Layer Web UI by clicking on the links generated in the output of the cell \n",
        "layer.run([create_dataset])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjQKa3w4TK0Y"
      },
      "source": [
        "Now, let's make some changes in the function body that affects the returning dataset's schema such as adding one more column. Assume that you have come up with a new feature that you think will help improve your ML model's prediction performance. You would add this new feature into your dataset in the next iteration (or experiment) and train your model using the new version of your dataset. In the case below, we will simulate it by increasing number of features ***n_features*** in the ***make_regression*** function from 3 to 4.<br>\n",
        "\n",
        "We copied the code of the function ***create_dataset*** and pasted it into the next cell and did that change accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DML7N0wM6S_3"
      },
      "outputs": [],
      "source": [
        "# Change your function create_dataset: Adding a new feature into the dataset and increasing number of features to 4. \n",
        "# See changes and compare the function with the one above.\n",
        "@dataset(\"my_first_dataset\")\n",
        "@assert_not_null([\"target\",\"feat1\"])\n",
        "@resources(path=\"./examples/comprehensive-getting-started/data\")\n",
        "def create_dataset():\n",
        "  from matplotlib import pyplot as plt\n",
        "  from sklearn.datasets import make_regression\n",
        "  import pandas as pd\n",
        "  \n",
        "  X_train, Y_train = make_regression(n_samples=100, n_features=4, n_targets=1, noise=0.5)\n",
        "\n",
        "  features = pd.DataFrame(X_train, columns = ['feat1', 'feat2', 'feat3','feat4'])\n",
        "  target = pd.DataFrame(Y_train, columns = ['target'])\n",
        "  training_df= pd.concat([features, target], axis=1)\n",
        "\n",
        "  external_df = pd.read_csv(\"examples/comprehensive-getting-started/data/external.csv\",names=['feat1','feat2','feat3','feat4','target'],header=None)\n",
        "  final_training_df = training_df.append(external_df)\n",
        "\n",
        "  training_df[['target']].plot(kind='box', title='Target Quartile Analysis')\n",
        "  layer.log({\"Box plots\": plt})\n",
        "\n",
        "  plt.close()\n",
        "  \n",
        "  return training_df "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiNj30apUDXe"
      },
      "source": [
        "Once again you run the function in the next cell after the changes, [Layer](https://layer.ai/) will create and register a new version of the same dataset but this time it will automatically bump up the major version (v1.2 --> v2.1) since there is a change in the schema of the ***my_first_dataset***. You will see a page similar to the one below when you click on the generated link.\n",
        "\n",
        "  <img src=\" https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-14%20at%2012.47.22.png?raw=true\" width=\"700\" height=\"500\" />\n",
        "\n",
        "Recall that we also logged a boxplot of the ***target*** column along with this dataset. You will see those logged data in the **Logged data** tab. Here is a screenshot taken from that tab.\n",
        "\n",
        "  <img src=\" https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-14%20at%2012.48.01.png?raw=true\" width=\"700\" height=\"500\" />\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ntq_NC4v8-nX"
      },
      "outputs": [],
      "source": [
        "# --- Run your function remotely ---\n",
        "\n",
        "# Run this cell once to create my_first_dataset v2.1\n",
        "# and check it out on Layer Web UI by clicking on the link generated in the output of the cell \n",
        "layer.run([create_dataset])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgErp95DZMU_"
      },
      "source": [
        "# **Step IV:** Learning more about Model Versioning on [Layer](https://layer.ai/)\n",
        "---\n",
        "\n",
        "Now, we are ready to fit our very first model on [Layer](https://layer.ai/) using the datasets generated in the previous step. [Layer](https://layer.ai/) will do automatic versioning on your models as well once you start building ML models by running your model train function several times.<br><br>\n",
        "This is how model versioning works on [Layer](https://layer.ai/):<br><br>\n",
        "**- If source code of a model is changed, then bump up the major version number**<br>\n",
        "*(e.g v2.2 → v3.1)*<br>\n",
        "**- Otherwise, bump up the minor version number**<br>\n",
        "*(e.g v2.2 → v2.3)*\n",
        "\n",
        "\n",
        "### [Layer](https://layer.ai/) Decorators and Functions Used In Step-IV\n",
        "\n",
        "---\n",
        "**@model:**<br>\n",
        "It is a decorator which defines the sklearn's Gradient Boosting Regressor model 'gbr' returned by the function as a [Layer](https://layer.ai/) model named my_first_model under the project my_first_project. Once you wrap your model's train function with this type of decorator, [Layer](https://layer.ai/) will start versioning your model automatically every time you run your function. You will also see a parameter named ***dependencies*** which expects a list of [Layer](https://layer.ai/) Datasets and Models that the function depends on. [Layer](https://layer.ai/) will optimise the build process accordingly. <br><br> \n",
        "For more information about the model decorator:<br>\n",
        "https://docs.app.layer.ai/docs/sdk-library/model-decorator<br><br>\n",
        "**@fabric:**<br>\n",
        "It is a special [Layer](https://layer.ai/) decorator that is used to set the type of remote machines and allocating different amount of CPU/GPU resources to run your function. In the case below, we will use the ***f-medium*** type machines to run our function ***create_model*** remotely. [Layer](https://layer.ai/) has many more pre-defined fabrics.<br><br>\n",
        "\n",
        "Check out other fabrics on [Layer](https://layer.ai/):<br>\n",
        "https://docs.app.layer.ai/docs/sdk-library/fabric-decorator<br><br>\n",
        "\n",
        "\n",
        "**@assert_true:**<br>\n",
        "This is a special type of assertion decorators which takes a custom function with a boolean return type as its parameter. Then, it will run that function and check whether the result returned by this function is true or not. As you see in the code snippet above, we have a test function named ***model_test_function*** which checks if train loss value is consistently decreasing in each iteration. It is expected that the train loss should decrease as new trees are added into ensemble. If you need to do such a sanity check on your model, then all you need to do is to give this function as a parameter into the assert_true decorator.<br><br>\n",
        "\n",
        "You could see more about the assert_true decorator here:<br>\n",
        "https://docs.app.layer.ai/docs/sdk-library/assert-true<br><br>\n",
        "\n",
        "**get_dataset():**<br>\n",
        "This function retrieves a [Layer](https://layer.ai/) dataset object from the Datasets you defined on [Layer](https://layer.ai/) previously. It returns a [Layer](https://layer.ai/) Dataset object and you should also call ***to_pandas()*** function to convert it to a Pandas data frame. In the case below, we fetches ***my_first_dataset v1.2*** to use as a training data to build the model.<br><br>\n",
        "\n",
        "Learn more about this function:<br>\n",
        "https://docs.app.layer.ai/docs/sdk-library/get-dataset<br><br>\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwIRYIcuz7oG"
      },
      "outputs": [],
      "source": [
        "# --- Create your first model ---\n",
        "\n",
        "# A user-defined function to test the model: Check if train loss is decreasing in each iteration\n",
        "def model_test_function(predictor):\n",
        "  score_arr = predictor.train_score_\n",
        "  for i in range( len(score_arr) - 1 ):\n",
        "    if score_arr[i] < score_arr[i+1]:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "# Model build function\n",
        "@model(\"my_first_model\" , dependencies=[Dataset('my_first_dataset')])\n",
        "@fabric(\"f-medium\")\n",
        "@assert_true(model_test_function)\n",
        "def create_model():\n",
        "  from sklearn.ensemble import GradientBoostingRegressor\n",
        "  from matplotlib import pyplot as plt\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn.metrics import mean_squared_error\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  \n",
        "  # Read dataset from Layer and split into train and test pandas dataframes\n",
        "  data = layer.get_dataset(\"my_first_dataset:1.2\").to_pandas()\n",
        "  X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data.target, random_state=42, test_size=0.1)\n",
        "\n",
        "  # Standardize the dataset\n",
        "  sc = StandardScaler()\n",
        "  X_train_std = sc.fit_transform(X_train)\n",
        "  X_test_std = sc.transform(X_test)\n",
        "\n",
        "  # Hyperparameters for GradientBoostingRegressor\n",
        "  gbr_params = {'n_estimators': 300,\n",
        "                'max_depth': 3,\n",
        "                'learning_rate': 0.1,\n",
        "                'min_samples_split' : 5,\n",
        "                'loss': 'ls'}\n",
        "\n",
        "  # Log model parameters to Layer\n",
        "  layer.log(gbr_params)\n",
        "\n",
        "  # Create an instance of gradient boosting regressor\n",
        "  gbr = GradientBoostingRegressor(**gbr_params)\n",
        "  # Fit the model\n",
        "  gbr.fit(X_train_std, y_train)\n",
        "\n",
        "  # Log loss scores incrementally using the step parameter\n",
        "  for i, y_pred in enumerate(gbr.staged_predict(X_test_std)):\n",
        "    loss = gbr.loss_(y_test, y_pred)\n",
        "    layer.log({\"Loss\":loss},step = i)\n",
        "\n",
        "  # Log Coefficient of determination R^2\n",
        "  r2 = gbr.score(X_test_std, y_test)\n",
        "  # Create the mean squared error\n",
        "  mse = mean_squared_error(y_test, gbr.predict(X_test_std))\n",
        "  # Get Feature importance data using feature_importances_ attribute\n",
        "  sorted_idx = gbr.feature_importances_.argsort()\n",
        "  plt.barh(data.iloc[:,:-1].columns[sorted_idx], gbr.feature_importances_[sorted_idx])\n",
        "  plt.xlabel(\"Gradient Boosting Regressor Feature Importance\")\n",
        "\n",
        "  # Log perfomance metrics and feature importance plot to Layer\n",
        "  layer.log({\"R2 Score\": r2,\n",
        "             \"Mean Squared Error\": mse,\n",
        "             \"Feature Importance Plot\": plt\n",
        "             })\n",
        "  \n",
        "  plt.close()\n",
        "\n",
        "  return gbr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TScoX0TcfZCM"
      },
      "source": [
        "Now, we are ready to execute our function by using the ***layer.run()***.<br>\n",
        "Once you run the cell below, it will create a very first version of your model and you could visit its page by clicking on the generated link in the cell's output. You will see a model page for your ***my_first_model v1.1*** like the one below:\n",
        "\n",
        "  <img src=\" https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-14%20at%2013.40.41.png?raw=true\" width=\"700\" height=\"500\" />\n",
        "\n",
        "You could also see some other data we logged along with the model by using ***layer.log*** function such as coefficients and a feature importance bar plot.<br>\n",
        "\n",
        "In the next experiment, run the cell below again without changing anything in the model's train function code. In this experiment, since there is no change in the model's function, [Layer](https://layer.ai/) train and register a model automatically and bump up only the minor version number *(v1.1 --> v1.2).*  This time, you will see the ***my_first_model v1.2*** page once you click on the generated link.\n",
        "\n",
        "  <img src=\" https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-14%20at%2013.41.17.png?raw=true\" width=\"700\" height=\"500\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C-Fa92A0dNw"
      },
      "outputs": [],
      "source": [
        "# --- Run your function remotely ---\n",
        "\n",
        "# Run this cell twice to create my_first_model v1.1 & v1.2 \n",
        "# and check them out on Layer Web UI by clicking on the links generated in the output of the cell\n",
        "layer.run([create_model])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICmc-YG1iX5Y"
      },
      "source": [
        "Let's copy the ***create_model*** function's code and paste it in the next cell as it is. Now, make some changes within the source code. For the sake of an easy practice, we will just change the ***learning_rate*** parameter from *0.01* to *0.1*. Since this is a change within the model function's source code, [Layer](https://layer.ai/) will detect it automatically and bump up the major version number *(v1.2 --> v2.1)* once you run the function again using ***layer.run()***. You will be directed to the ***my_first_model v2.1***'s page when you click on generated link.  \n",
        "\n",
        "<img src=\" https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-14%20at%2013.58.15.png?raw=true\" width=\"700\" height=\"500\" />\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THRMgtvq0p8T"
      },
      "outputs": [],
      "source": [
        "# --- Change the learning_rate parameter ---\n",
        "\n",
        "# A user-defined function to test the model: Check if train loss is decresed in each iteration step\n",
        "def model_test_function(predictor):\n",
        "  score_arr = predictor.train_score_\n",
        "  for i in range( len(score_arr) - 1 ):\n",
        "    if score_arr[i] < score_arr[i+1]:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "# Model build function\n",
        "@model(\"my_first_model\" , dependencies=[Dataset('my_first_dataset')])\n",
        "@fabric(\"f-medium\")\n",
        "@assert_true(model_test_function)\n",
        "def create_model():\n",
        "  from sklearn.ensemble import GradientBoostingRegressor\n",
        "  from matplotlib import pyplot as plt\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn.metrics import mean_squared_error\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  \n",
        "  # Read dataset from Layer and split into train and test pandas dataframes\n",
        "  data = layer.get_dataset(\"my_first_dataset:1.2\").to_pandas()\n",
        "  X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data.target, random_state=42, test_size=0.1)\n",
        "\n",
        "  # Standardize the dataset\n",
        "  sc = StandardScaler()\n",
        "  X_train_std = sc.fit_transform(X_train)\n",
        "  X_test_std = sc.transform(X_test)\n",
        "\n",
        "  # Hyperparameters for GradientBoostingRegressor\n",
        "  gbr_params = {'n_estimators': 300,\n",
        "                'max_depth': 3,\n",
        "                # CHANGED HERE: 0.1 --> 0.01 \n",
        "                'learning_rate': 0.01,\n",
        "                'min_samples_split' : 5,\n",
        "                'loss': 'ls'}\n",
        "\n",
        "  # Log model parameters to Layer\n",
        "  layer.log(gbr_params)\n",
        "\n",
        "  # Create an instance of gradient boosting regressor\n",
        "  gbr = GradientBoostingRegressor(**gbr_params)\n",
        "  # Fit the model\n",
        "  gbr.fit(X_train_std, y_train)\n",
        "\n",
        "  # Log loss scores incrementally using the step parameter\n",
        "  for i, y_pred in enumerate(gbr.staged_predict(X_test_std)):\n",
        "    loss = gbr.loss_(y_test, y_pred)\n",
        "    layer.log({\"Loss\":loss},step = i)\n",
        "\n",
        "  # Log Coefficient of determination R^2\n",
        "  r2 = gbr.score(X_test_std, y_test)\n",
        "  # Create the mean squared error\n",
        "  mse = mean_squared_error(y_test, gbr.predict(X_test_std))\n",
        "  # Get Feature importance data using feature_importances_ attribute\n",
        "  sorted_idx = gbr.feature_importances_.argsort()\n",
        "  plt.barh(data.iloc[:,:-1].columns[sorted_idx], gbr.feature_importances_[sorted_idx])\n",
        "  plt.xlabel(\"Gradient Boosting Regressor Feature Importance\")\n",
        "\n",
        "  # Log perfomance metrics and feature importance plot to Layer\n",
        "  layer.log({\"R2 Score\": r2,\n",
        "             \"Mean Squared Error\": mse,\n",
        "             \"Feature Importance Plot\": plt\n",
        "             })\n",
        "  \n",
        "  plt.close()\n",
        "\n",
        "  return gbr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI3amdVP9HdZ"
      },
      "outputs": [],
      "source": [
        "# --- Run your function remotely ---\n",
        "\n",
        "# Run this cell once to create 'my_first_model v2.1' \n",
        "# and check it out on Layer Web UI by clicking on the link generated in the output of the cell\n",
        "layer.run([create_model])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOTi-jCvkUl4"
      },
      "source": [
        "\n",
        "\n",
        "Before wrapping up the model part, let me show you 2 more important actions you could do in this page.<br>\n",
        "\n",
        "**- Compare model versions**\n",
        "\n",
        "When you select multiple model versions on the left panel, you will see all the logged data belongs to these versions side-by-side. You could also copy links to these plots to share with someone easily or put it into project readme file as we will described in a following section. In the screenshot below, we compare the model ***my_first_model v2.1*** with its previous ***v1.2*** version.\n",
        "\n",
        "<img src=\" https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-14%20at%2014.00.24.png?raw=true\" width=\"700\" height=\"500\" />\n",
        "\n",
        "**- Share and import a model**\n",
        "\n",
        "If you would like to share or import a specific version of your ML model into another project or your notebook, you could click on the **'</> Use from Layer'** button and copy the code snippet. You could do the same for datasets as well.\n",
        "\n",
        "<img src=\" https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-14%20at%2014.00.55.png?raw=true\" width=\"700\" height=\"500\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byK9AhG2nu-c"
      },
      "source": [
        "# **Step V:** Run multiple-functions experiments on [Layer](https://layer.ai/)\n",
        "---\n",
        "\n",
        "Let's create one more [Layer](https://layer.ai/) dataset which holds predictions we get on a seperate test data using a previously trained and registered ***my_first_model v2.1*** model.<br>\n",
        "\n",
        "### [Layer](https://layer.ai/) Decorators and Functions Used In Step-V\n",
        "\n",
        "---\n",
        "**get_model():**<br>\n",
        "This function retrieves a [Layer](https://layer.ai/) model object from the Models you registered on [Layer](https://layer.ai/) previously. It returns a [Layer](https://layer.ai/) Model object and you should also call ***get_train()*** function to convert it to a regular sklearn model type. In the case below, we fetches ***my_first_model v2.1*** to make predictions on the test data.<br><br>\n",
        "\n",
        "Learn more about this function:<br>\n",
        "https://docs.app.layer.ai/docs/sdk-library/get-model<br><br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rR43envC0Lv3"
      },
      "outputs": [],
      "source": [
        "# Create another dataset named 'predictions' by fetching the registered my_first_model v2.1 on Layer\n",
        "@dataset(\"predictions\", dependencies=[Model('my_first_model')])\n",
        "def generate_predictions():\n",
        "  from sklearn.datasets import make_regression\n",
        "  import pandas as pd\n",
        "  from sklearn.metrics import r2_score\n",
        "  \n",
        "  X_test, _ = make_regression(n_samples=10, n_features=3, n_targets=0, noise=0.5)\n",
        "\n",
        "  X_test = pd.DataFrame(X_test, columns = ['feat1', 'feat2', 'feat3'])\n",
        "  \n",
        "  my_model = layer.get_model(\"my_first_model:2.1\").get_train()\n",
        "  Y_pred_arr = my_model.predict(X_test)\n",
        "  Y_pred = pd.DataFrame(Y_pred_arr, columns = ['prediction'])\n",
        "\n",
        "  predictions_df = pd.concat([X_test, Y_pred], axis=1)\n",
        "\n",
        "  return predictions_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbI2BDxjP4od"
      },
      "outputs": [],
      "source": [
        "# --- Run your function remotely ---\n",
        "\n",
        "# Run this cell once to create dataset 'predictions v1.1' \n",
        "# and check it out on Layer Web UI by clicking on the link generated in the output of the cell \n",
        "layer.run([generate_predictions])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TKSlr5fwLtZ"
      },
      "source": [
        "---\n",
        "We have defined 3 different functions so far:\n",
        "- create_dataset()\n",
        "- create_model()\n",
        "- generate_predictions()\n",
        "\n",
        "So far, we have used these functions singular in the ***layer.run()*** function and run each of them one by one. However, we can also have multiple functions within the ***layer.run()*** and run them in an order. The order these functions will be running is determined by [Layer](https://layer.ai/) automatically based on dependencies between these functions so that you don't have to explicitly state it. [Layer](https://layer.ai/) will run functions in parallel whenever possible to optimise running time. Now, let's run these 3 functions all together in a single run by executing the single line of code below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrULNiAJwAQf"
      },
      "source": [
        "[Layer](https://layer.ai/) will run these functions in this exact order: ***create_dataset*** → ***create_model*** → ***generate_predictions*** given the ***dependencies*** parameters. It is because the ***create_model*** function uses a dataset generated by the ***create_dataset*** function and the ***generate_predictions*** function uses a model created by the ***create_model*** function. Once it is done, you will end up with newer versions of your datasets ***my_first_dataset*** and ***predictions*** as well as a new version of your model ***my_first_model***.<br><br> \n",
        "To see all the experiments we have run so far, you should click on the ***'Runs'*** tab on the project page and see the list of runs and entities generated by each respective run. As you will see at top of the page below, the latest run creates new versions of 2 datasets: ***my_first_dataset v2.2*** & ***predictions v1.2*** and 1 model: ***my_first_model v2.2***. However, the second run from the top creates only the very first version of the ***predictions*** dataset. You could also click on those entities to go their respective pages to get more details.\n",
        "\n",
        "<img src=\" https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-14%20at%2015.00.32.png?raw=true\" width=\"700\" height=\"500\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8uAGiuV932N"
      },
      "outputs": [],
      "source": [
        "# --- Run multiple functions remotely ---\n",
        "\n",
        "# Run this cell once and then check out the 'Runs' tab to see list of runs we've had so far.\n",
        "layer.run([create_dataset,create_model,generate_predictions])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQv64Z5D2MJz"
      },
      "source": [
        "# **Step VI:** Upload and Create Dynamic Project Documentation\n",
        "---\n",
        "\n",
        "Another useful feature of [Layer](https://layer.ai/) is to let users to have dynamic project readme documentations. It is basically a regular markdown file. However, you could also add links to datasets, models or any logged data into these files. [Layer](https://layer.ai/) will show these links as clickable entity cards where people can go to their respective pages once they click on them. That's what makes these documentations 'dynamic' and interactive.<br><br>\n",
        "All you need is to create a project readme markdown file by using your favourite text editor and put it into the working directory on your Google Colab notebook. Once you upload that file and run the ***layer.init*** function with the same project name again. [Layer](https://layer.ai/) will render and show it in the project main page similar to the screenshot below.\n",
        "\n",
        "We already created a README.md file for you. It is located under the directory */content/examples/comprehensive-getting-started/* All you have to do is to make sure it is moved in the project root directory which is */content* for Google Colab users.\n",
        "\n",
        "Check out the page to learn more about Project cards on Layer:<br>\n",
        "https://docs.app.layer.ai/docs/projects/card <br><br>\n",
        "\n",
        "\n",
        "<img src=\" https://github.com/layerai/examples/blob/main/comprehensive-getting-started/images/Screenshot%202022-07-14%20at%2015.05.48.png?raw=true\" width=\"450\" height=\"600\" />\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odENGqP40Ae4"
      },
      "outputs": [],
      "source": [
        "# --- Run this cell once after you upload your README.md file into your project working directory ---\n",
        "\n",
        "layer.init(\"my-first-project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JvG2yVNm_nS"
      },
      "source": [
        "# **Step VII:** Run [Layer](https://layer.ai/) in local mode\n",
        "---\n",
        "\n",
        "In order to use [Layer](https://layer.ai/) to store your ML metadata, you don't have use the ***layer.run()*** to run your functions remotely on the [Layer](https://layer.ai/) Cloud. You could also use your own resources to run your functions locally and still ask [Layer](https://layer.ai/) to store results returned from your functions. <br><br>\n",
        "\n",
        "In the cell below, we just call functions locally in their correct order and [Layer](https://layer.ai/) will still store generated results returned from those functions. Click on the respective links for each entity to see them on the [Layer](https://layer.ai/) Web UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8-m2loGoseQ"
      },
      "outputs": [],
      "source": [
        "# --- Run your functions locally using your own resources and machines\n",
        "\n",
        "# my_first_dataset\n",
        "create_dataset()\n",
        "\n",
        "# my_first_model\n",
        "create_model()\n",
        "\n",
        "# predictions\n",
        "generate_predictions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CONCLUSION\n",
        "\n",
        "Congratulations, you are done! You have completed the comprehensive Layer Demo by building an end-to-end ML pipeline.<br><br>\n",
        "\n",
        "To learn more about using Layer, you can:\n",
        "\n",
        "- Join our [Slack Community](https://layer-community.slack.com/)\n",
        "- Visit [Layer Examples Repo](https://github.com/layerai/examples) for more examples\n",
        "- Browse Trending Layer Projects on our [mainpage](https://layer.ai/)\n",
        "- Check out [Layer Documentation](https://docs.app.layer.ai/docs/) to learn more"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Layer Demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
