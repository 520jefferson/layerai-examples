{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Question Answering with Hugging Face Transformers](https://keras.io/examples/nlp/question_answering/)","metadata":{"id":"4gaI_RYe0XPC"}},{"cell_type":"code","source":"!pip install transformers\n!pip install datasets\n!pip install huggingface-hub","metadata":{"id":"WrhSroQE0XPE","outputId":"32564ca2-1f96-4fbd-bfd7-79951d413b57","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install layer-sdk -qqq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import layer\nfrom layer.decorators import model, resources, pip_requirements, fabric, dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:49:37.197194Z","iopub.execute_input":"2022-04-01T10:49:37.197689Z","iopub.status.idle":"2022-04-01T10:49:37.539899Z","shell.execute_reply.started":"2022-04-01T10:49:37.197603Z","shell.execute_reply":"2022-04-01T10:49:37.539135Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# layer.logout()\n# layer.login(\"https://app.layer.ai/\")\nlayer.login(\"https://development.layer.co/\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:49:54.919647Z","iopub.execute_input":"2022-04-01T10:49:54.920150Z","iopub.status.idle":"2022-04-01T10:49:54.926389Z","shell.execute_reply.started":"2022-04-01T10:49:54.920110Z","shell.execute_reply":"2022-04-01T10:49:54.925606Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"layer.init(\"qas\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:49:55.121113Z","iopub.execute_input":"2022-04-01T10:49:55.121375Z","iopub.status.idle":"2022-04-01T10:49:56.603517Z","shell.execute_reply.started":"2022-04-01T10:49:55.121347Z","shell.execute_reply":"2022-04-01T10:49:56.602711Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def prepare_train_features(examples):\n    # Tokenize our examples with truncation and padding, but keep the overflows using a\n    # stride. This results in one example possible giving several features when a context is long,\n    # each of those features having a context that overlaps a bit the context of the previous\n    # feature.\n    from transformers import AutoTokenizer\n    model_checkpoint = \"distilbert-base-uncased\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    max_length = 384  # The maximum length of a feature (question and context)\n    doc_stride = (\n        128  # The authorized overlap between two part of the context when splitting\n    )\n# it is needed.\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n    examples[\"context\"] = [c.lstrip() for c in examples[\"context\"]]\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a\n    # map from a feature to its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original\n    # context. This will help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what\n        # is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this\n        # span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the\n            # CLS index).\n            if not (\n                offsets[token_start_index][0] <= start_char\n                and offsets[token_end_index][1] >= end_char\n            ):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the\n                # answer.\n                # Note: we could go after the last offset if the answer is the last word (edge\n                # case).\n                while (\n                    token_start_index < len(offsets)\n                    and offsets[token_start_index][0] <= start_char\n                ):\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@fabric(\"f-medium\")\n@pip_requirements(packages=[\"transformers\",\"tensorflow\",\"keras\",\"sentencepiece\",\"datasets\"])\n@model(\"qa\")\ndef train():\n    import tensorflow as tf\n    from tensorflow import keras\n    from transformers import TFAutoModelForQuestionAnswering\n    from datasets import load_dataset\n    datasets = load_dataset(\"squad\")\n    tokenized_datasets = datasets.map(prepare_train_features,batched=True,remove_columns=datasets[\"train\"].column_names,num_proc=3,)\n    train_set = tokenized_datasets[\"train\"].with_format(\"numpy\")[:]\n    # Load the whole dataset as a dict of numpy arrays\n    validation_set = tokenized_datasets[\"validation\"].with_format(\"numpy\")[:]\n    model_checkpoint = \"distilbert-base-uncased\"\n    model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n    optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n  # Optionally uncomment the next line for float16 training\n    keras.mixed_precision.set_global_policy(\"mixed_float16\")\n    model.compile(optimizer=optimizer)\n    model.fit(train_set, validation_data=validation_set, epochs=1)\n    return model","metadata":{"id":"514dIhjZ0XPG","outputId":"b468be90-0585-4747-eac0-3678e97f50c0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = layer.get_model(\"qa\").get_train()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:50:01.781912Z","iopub.execute_input":"2022-04-01T10:50:01.782633Z","iopub.status.idle":"2022-04-01T10:50:14.933060Z","shell.execute_reply.started":"2022-04-01T10:50:01.782583Z","shell.execute_reply":"2022-04-01T10:50:14.932283Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport tensorflow as tf\nmodel_checkpoint = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\ncontext = \"\"\"\nOn a mild day in late July, a teenager was walking home from the downtown\narea, such as it was, of Little Whinging. The city did not have a train station, a\ndepartment store, or even a large supermarket; just a few dozen shops, a bank, a\npolice station, and a library. It was the library from which soon-to-be-sixteen-yearold Harry Potter was returning, with an old bookbag around his shoulder. He\nlooked around from time to time as he walked. It looked as though he were\nadmiring the trees and bushes, which had recovered nicely from last year’s drought,\nbut he actually was wondering whether there was anyone following him. Or, more\nprecisely, whether he could catch a glimpse of the person he knew must be\nfollowing him. All he could see, however, were the normal sights of a suburban\nneighborhood, and a few people looking at him rather oddly as they passed him.\nHarry briefly wondered why–after all, he was not exactly famous in this area, nor\nwas his scar–until he realized that looking around to see if you were being followed\nwas not exactly usual behavior. \n\"\"\"","metadata":{"id":"0b30CGoa0XPG","execution":{"iopub.status.busy":"2022-04-01T10:50:14.936663Z","iopub.execute_input":"2022-04-01T10:50:14.936883Z","iopub.status.idle":"2022-04-01T10:50:18.028041Z","shell.execute_reply.started":"2022-04-01T10:50:14.936856Z","shell.execute_reply":"2022-04-01T10:50:18.027240Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"question = \"When was the teenegar walking home?\"\ninputs = tokenizer([context], [question], return_tensors=\"np\")\noutputs = model(inputs)\nstart_position = tf.argmax(outputs.start_logits, axis=1)\nend_position = tf.argmax(outputs.end_logits, axis=1)\nprint(int(start_position), int(end_position[0]))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:50:18.029436Z","iopub.execute_input":"2022-04-01T10:50:18.029870Z","iopub.status.idle":"2022-04-01T10:50:18.099361Z","shell.execute_reply.started":"2022-04-01T10:50:18.029832Z","shell.execute_reply":"2022-04-01T10:50:18.098489Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"answer = inputs[\"input_ids\"][0, int(start_position) : int(end_position) + 1]\nprint(answer)","metadata":{"id":"i2CIqv_u0XPG","execution":{"iopub.status.busy":"2022-04-01T10:50:18.101001Z","iopub.execute_input":"2022-04-01T10:50:18.101247Z","iopub.status.idle":"2022-04-01T10:50:18.108079Z","shell.execute_reply.started":"2022-04-01T10:50:18.101211Z","shell.execute_reply":"2022-04-01T10:50:18.106083Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.decode(answer))","metadata":{"id":"Yd9X2EvK0XPG","execution":{"iopub.status.busy":"2022-04-01T10:50:22.228005Z","iopub.execute_input":"2022-04-01T10:50:22.228606Z","iopub.status.idle":"2022-04-01T10:50:22.739820Z","shell.execute_reply.started":"2022-04-01T10:50:22.228563Z","shell.execute_reply":"2022-04-01T10:50:22.739053Z"},"trusted":true},"execution_count":8,"outputs":[]}]}