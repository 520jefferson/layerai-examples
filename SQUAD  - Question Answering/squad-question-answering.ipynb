{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Question Answering with Hugging Face Transformers](https://keras.io/examples/nlp/question_answering/)","metadata":{"id":"4gaI_RYe0XPC"}},{"cell_type":"code","source":"!pip install transformers\n!pip install datasets\n!pip install huggingface-hub","metadata":{"id":"WrhSroQE0XPE","outputId":"32564ca2-1f96-4fbd-bfd7-79951d413b57","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install layer-sdk -qqq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import layer\nfrom layer.decorators import model, resources, pip_requirements, fabric, dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:00:08.937360Z","iopub.execute_input":"2022-04-01T09:00:08.937694Z","iopub.status.idle":"2022-04-01T09:00:08.942707Z","shell.execute_reply.started":"2022-04-01T09:00:08.937662Z","shell.execute_reply":"2022-04-01T09:00:08.941577Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# layer.logout()\n# layer.login(\"https://app.layer.ai/\")\nlayer.login(\"https://development.layer.co/\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:00:09.136285Z","iopub.execute_input":"2022-04-01T09:00:09.136956Z","iopub.status.idle":"2022-04-01T09:00:22.538145Z","shell.execute_reply.started":"2022-04-01T09:00:09.136922Z","shell.execute_reply":"2022-04-01T09:00:22.536995Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"layer.init(\"qas\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:00:25.263631Z","iopub.execute_input":"2022-04-01T09:00:25.263919Z","iopub.status.idle":"2022-04-01T09:00:25.697159Z","shell.execute_reply.started":"2022-04-01T09:00:25.263888Z","shell.execute_reply":"2022-04-01T09:00:25.695942Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def prepare_train_features(examples):\n    # Tokenize our examples with truncation and padding, but keep the overflows using a\n    # stride. This results in one example possible giving several features when a context is long,\n    # each of those features having a context that overlaps a bit the context of the previous\n    # feature.\n    from transformers import AutoTokenizer\n    model_checkpoint = \"distilbert-base-uncased\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    max_length = 384  # The maximum length of a feature (question and context)\n    doc_stride = (\n        128  # The authorized overlap between two part of the context when splitting\n    )\n# it is needed.\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n    examples[\"context\"] = [c.lstrip() for c in examples[\"context\"]]\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a\n    # map from a feature to its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original\n    # context. This will help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what\n        # is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this\n        # span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the\n            # CLS index).\n            if not (\n                offsets[token_start_index][0] <= start_char\n                and offsets[token_end_index][1] >= end_char\n            ):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the\n                # answer.\n                # Note: we could go after the last offset if the answer is the last word (edge\n                # case).\n                while (\n                    token_start_index < len(offsets)\n                    and offsets[token_start_index][0] <= start_char\n                ):\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:00:27.378960Z","iopub.execute_input":"2022-04-01T09:00:27.379301Z","iopub.status.idle":"2022-04-01T09:00:27.399810Z","shell.execute_reply.started":"2022-04-01T09:00:27.379238Z","shell.execute_reply":"2022-04-01T09:00:27.398742Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"@fabric(\"f-medium\")\n@pip_requirements(packages=[\"transformers\",\"tensorflow\",\"keras\",\"sentencepiece\",\"datasets\"])\n@model(\"qa\")\ndef train():\n    import tensorflow as tf\n    from tensorflow import keras\n    from transformers import TFAutoModelForQuestionAnswering\n    from datasets import load_dataset\n    datasets = load_dataset(\"squad\")\n    tokenized_datasets = datasets.map(prepare_train_features,batched=True,remove_columns=datasets[\"train\"].column_names,num_proc=3,)\n    train_set = tokenized_datasets[\"train\"].with_format(\"numpy\")[:]\n    # Load the whole dataset as a dict of numpy arrays\n    validation_set = tokenized_datasets[\"validation\"].with_format(\"numpy\")[:]\n    model_checkpoint = \"distilbert-base-uncased\"\n    model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n    optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n  # Optionally uncomment the next line for float16 training\n    keras.mixed_precision.set_global_policy(\"mixed_float16\")\n    model.compile(optimizer=optimizer)\n    model.fit(train_set, validation_data=validation_set, epochs=1)\n    return model","metadata":{"id":"514dIhjZ0XPG","outputId":"b468be90-0585-4747-eac0-3678e97f50c0","execution":{"iopub.status.busy":"2022-04-01T09:00:28.491095Z","iopub.execute_input":"2022-04-01T09:00:28.491833Z","iopub.status.idle":"2022-04-01T09:00:28.505124Z","shell.execute_reply.started":"2022-04-01T09:00:28.491787Z","shell.execute_reply":"2022-04-01T09:00:28.503926Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:00:29.162980Z","iopub.execute_input":"2022-04-01T09:00:29.163255Z","iopub.status.idle":"2022-04-01T09:41:31.370157Z","shell.execute_reply.started":"2022-04-01T09:00:29.163223Z","shell.execute_reply":"2022-04-01T09:41:31.369119Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = layer.get_model(\"qa\").get_train()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:41:31.375719Z","iopub.execute_input":"2022-04-01T09:41:31.376526Z","iopub.status.idle":"2022-04-01T09:41:34.891272Z","shell.execute_reply.started":"2022-04-01T09:41:31.376478Z","shell.execute_reply":"2022-04-01T09:41:34.890379Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport tensorflow as tf\nmodel_checkpoint = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\ncontext = \"\"\"Keras is an API designed for human beings, not machines. Keras follows best\npractices for reducing cognitive load: it  offers consistent & simple APIs, it minimizes\nthe number of user actions required for common use cases, and it provides clear &\nactionable error messages. It also has extensive documentation and developer guides. \"\"\"\nquestion = \"What is Keras?\"\n\ninputs = tokenizer([context], [question], return_tensors=\"np\")\noutputs = model(inputs)\nstart_position = tf.argmax(outputs.start_logits, axis=1)\nend_position = tf.argmax(outputs.end_logits, axis=1)\nprint(int(start_position), int(end_position[0]))","metadata":{"id":"0b30CGoa0XPG","execution":{"iopub.status.busy":"2022-04-01T09:42:54.331849Z","iopub.execute_input":"2022-04-01T09:42:54.332144Z","iopub.status.idle":"2022-04-01T09:42:58.456355Z","shell.execute_reply.started":"2022-04-01T09:42:54.332104Z","shell.execute_reply":"2022-04-01T09:42:58.455392Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"answer = inputs[\"input_ids\"][0, int(start_position) : int(end_position) + 1]\nprint(answer)","metadata":{"id":"i2CIqv_u0XPG","execution":{"iopub.status.busy":"2022-04-01T09:42:59.483241Z","iopub.execute_input":"2022-04-01T09:42:59.484209Z","iopub.status.idle":"2022-04-01T09:42:59.491259Z","shell.execute_reply.started":"2022-04-01T09:42:59.484176Z","shell.execute_reply":"2022-04-01T09:42:59.490009Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.decode(answer))","metadata":{"id":"Yd9X2EvK0XPG","execution":{"iopub.status.busy":"2022-04-01T09:42:59.951331Z","iopub.execute_input":"2022-04-01T09:42:59.951604Z","iopub.status.idle":"2022-04-01T09:42:59.958003Z","shell.execute_reply.started":"2022-04-01T09:42:59.951574Z","shell.execute_reply":"2022-04-01T09:42:59.956892Z"},"trusted":true},"execution_count":13,"outputs":[]}]}