{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "name": "squad -question-answering.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [Question Answering with Hugging Face Transformers](https://keras.io/examples/nlp/question_answering/)\n",
    "In this notebook we fine tune a Hugging Face BERT model on the [SQUAD dataset](https://huggingface.co/datasets/squad)\n",
    "for question answering."
   ],
   "metadata": {
    "id": "4gaI_RYe0XPC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[![Open in Layer](https://development.layer.co/assets/badge.svg)](https://development.layer.co/layer/qas/models/qa) [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1h8M_sLKAbvsAA11qgSPsOc98g08RB4_u/view?usp=sharing) [![Layer Examples Github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/layerai/examples)"
   ],
   "metadata": {
    "id": "HOSYuD9B4afI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install huggingface-hub"
   ],
   "metadata": {
    "id": "WrhSroQE0XPE",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install layer-sdk -qqq"
   ],
   "metadata": {
    "trusted": true,
    "id": "zOfVMgPz3-PS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import layer\n",
    "from layer.decorators import model, resources, pip_requirements, fabric, dataset"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-01T11:16:12.522769Z",
     "iopub.execute_input": "2022-04-01T11:16:12.523521Z",
     "iopub.status.idle": "2022-04-01T11:16:12.529816Z",
     "shell.execute_reply.started": "2022-04-01T11:16:12.523480Z",
     "shell.execute_reply": "2022-04-01T11:16:12.529043Z"
    },
    "trusted": true,
    "id": "MAxVxnDb3-PS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "layer.login(\"https://development.layer.co/\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-01T11:13:44.104262Z",
     "iopub.execute_input": "2022-04-01T11:13:44.104531Z",
     "iopub.status.idle": "2022-04-01T11:13:44.112010Z",
     "shell.execute_reply.started": "2022-04-01T11:13:44.104501Z",
     "shell.execute_reply": "2022-04-01T11:13:44.111078Z"
    },
    "trusted": true,
    "id": "u4pbsA8V3-PT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "layer.init(\"qas\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-01T11:13:45.435576Z",
     "iopub.execute_input": "2022-04-01T11:13:45.436105Z",
     "iopub.status.idle": "2022-04-01T11:13:46.888429Z",
     "shell.execute_reply.started": "2022-04-01T11:13:45.436065Z",
     "shell.execute_reply": "2022-04-01T11:13:46.887735Z"
    },
    "trusted": true,
    "id": "-o0zWboU3-PT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a\n",
    "    # stride. This results in one example possible giving several features when a context is long,\n",
    "    # each of those features having a context that overlaps a bit the context of the previous\n",
    "    # feature.\n",
    "    from transformers import AutoTokenizer\n",
    "    model_checkpoint = \"distilbert-base-uncased\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    max_length = 384  # The maximum length of a feature (question and context)\n",
    "    doc_stride = (\n",
    "        128  # The authorized overlap between two part of the context when splitting\n",
    "    )\n",
    "# it is needed.\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    examples[\"context\"] = [c.lstrip() for c in examples[\"context\"]]\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a\n",
    "    # map from a feature to its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original\n",
    "    # context. This will help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what\n",
    "        # is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this\n",
    "        # span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the\n",
    "            # CLS index).\n",
    "            if not (\n",
    "                offsets[token_start_index][0] <= start_char\n",
    "                and offsets[token_end_index][1] >= end_char\n",
    "            ):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the\n",
    "                # answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge\n",
    "                # case).\n",
    "                while (\n",
    "                    token_start_index < len(offsets)\n",
    "                    and offsets[token_start_index][0] <= start_char\n",
    "                ):\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ],
   "metadata": {
    "trusted": true,
    "id": "QQpXC5j43-PT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"squad\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-01T11:18:48.958184Z",
     "iopub.execute_input": "2022-04-01T11:18:48.958528Z",
     "iopub.status.idle": "2022-04-01T11:18:50.025448Z",
     "shell.execute_reply.started": "2022-04-01T11:18:48.958490Z",
     "shell.execute_reply": "2022-04-01T11:18:50.024754Z"
    },
    "trusted": true,
    "id": "pgBOi7iu3-PU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(datasets[\"train\"][0])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-01T11:19:19.984101Z",
     "iopub.execute_input": "2022-04-01T11:19:19.984913Z",
     "iopub.status.idle": "2022-04-01T11:19:19.991659Z",
     "shell.execute_reply.started": "2022-04-01T11:19:19.984865Z",
     "shell.execute_reply": "2022-04-01T11:19:19.990826Z"
    },
    "trusted": true,
    "id": "jSW9GTV_3-PV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@fabric(\"f-medium\")\n",
    "@pip_requirements(packages=[\"transformers\",\"tensorflow\",\"keras\",\"sentencepiece\",\"datasets\"])\n",
    "@model(\"qa\")\n",
    "def fine_tune():\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from transformers import TFAutoModelForQuestionAnswering\n",
    "    from datasets import load_dataset\n",
    "    datasets = load_dataset(\"squad\")\n",
    "    tokenized_datasets = datasets.map(prepare_train_features,batched=True,remove_columns=datasets[\"train\"].column_names,num_proc=3,)\n",
    "    train_set = tokenized_datasets[\"train\"].with_format(\"numpy\")[:]\n",
    "    # Load the whole dataset as a dict of numpy arrays\n",
    "    validation_set = tokenized_datasets[\"validation\"].with_format(\"numpy\")[:]\n",
    "    model_checkpoint = \"distilbert-base-uncased\"\n",
    "    model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n",
    "  # Optionally uncomment the next line for float16 training\n",
    "    keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    model.compile(optimizer=optimizer)\n",
    "    model.fit(train_set, validation_data=validation_set, epochs=1)\n",
    "    return model"
   ],
   "metadata": {
    "id": "514dIhjZ0XPG",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train()"
   ],
   "metadata": {
    "trusted": true,
    "id": "y96Rw1Yu3-PV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@fabric(\"f-medium\")\n",
    "@pip_requirements(packages=[\"transformers\"])\n",
    "@model(\"dsbt-tokenizer\")\n",
    "def save_tokenizer():\n",
    "    from transformers import DistilBertTokenizer\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    return tokenizer"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-01T11:16:20.153481Z",
     "iopub.execute_input": "2022-04-01T11:16:20.153754Z",
     "iopub.status.idle": "2022-04-01T11:16:20.160960Z",
     "shell.execute_reply.started": "2022-04-01T11:16:20.153722Z",
     "shell.execute_reply": "2022-04-01T11:16:20.159751Z"
    },
    "trusted": true,
    "id": "r3HEexl53-PV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "save_tokenizer()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-01T11:16:21.636068Z",
     "iopub.execute_input": "2022-04-01T11:16:21.636349Z",
     "iopub.status.idle": "2022-04-01T11:16:30.988562Z",
     "shell.execute_reply.started": "2022-04-01T11:16:21.636318Z",
     "shell.execute_reply": "2022-04-01T11:16:30.987713Z"
    },
    "trusted": true,
    "id": "LJBOtWMc3-PW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = layer.get_model(\"qa\").get_train()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-01T11:16:34.499699Z",
     "iopub.execute_input": "2022-04-01T11:16:34.500748Z",
     "iopub.status.idle": "2022-04-01T11:16:41.629709Z",
     "shell.execute_reply.started": "2022-04-01T11:16:34.500695Z",
     "shell.execute_reply": "2022-04-01T11:16:41.628752Z"
    },
    "trusted": true,
    "id": "XH4RQPa53-PW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "context = \"\"\"\n",
    "On a mild day in late July, a teenager was walking home from the downtown\n",
    "area, such as it was, of Little Whinging. The city did not have a train station, a\n",
    "department store, or even a large supermarket; just a few dozen shops, a bank, a\n",
    "police station, and a library. It was the library from which soon-to-be-sixteen-yearold Harry Potter was returning, with an old bookbag around his shoulder. He\n",
    "looked around from time to time as he walked. It looked as though he were\n",
    "admiring the trees and bushes, which had recovered nicely from last year’s drought,\n",
    "but he actually was wondering whether there was anyone following him. Or, more\n",
    "precisely, whether he could catch a glimpse of the person he knew must be\n",
    "following him. All he could see, however, were the normal sights of a suburban\n",
    "neighborhood, and a few people looking at him rather oddly as they passed him.\n",
    "Harry briefly wondered why–after all, he was not exactly famous in this area, nor\n",
    "was his scar–until he realized that looking around to see if you were being followed\n",
    "was not exactly usual behavior. \n",
    "\"\"\""
   ],
   "metadata": {
    "id": "0b30CGoa0XPG",
    "execution": {
     "iopub.status.busy": "2022-04-01T11:16:45.784164Z",
     "iopub.execute_input": "2022-04-01T11:16:45.784902Z",
     "iopub.status.idle": "2022-04-01T11:16:45.792319Z",
     "shell.execute_reply.started": "2022-04-01T11:16:45.784858Z",
     "shell.execute_reply": "2022-04-01T11:16:45.791529Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "tokenizer = layer.get_model(\"dsbt-tokenizer\").get_train()\n",
    "question = \"When was the teenegar walking home?\"\n",
    "inputs = tokenizer([context], [question], return_tensors=\"np\")\n",
    "outputs = model(inputs)\n",
    "start_position = tf.argmax(outputs.start_logits, axis=1)\n",
    "end_position = tf.argmax(outputs.end_logits, axis=1)\n",
    "print(int(start_position), int(end_position[0]))\n",
    "answer = inputs[\"input_ids\"][0, int(start_position) : int(end_position) + 1]\n",
    "print(answer)\n",
    "print(tokenizer.decode(answer))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-01T11:17:51.800736Z",
     "iopub.execute_input": "2022-04-01T11:17:51.801507Z",
     "iopub.status.idle": "2022-04-01T11:17:55.952248Z",
     "shell.execute_reply.started": "2022-04-01T11:17:51.801466Z",
     "shell.execute_reply": "2022-04-01T11:17:55.950753Z"
    },
    "trusted": true,
    "id": "hdbfI4KW3-PW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}