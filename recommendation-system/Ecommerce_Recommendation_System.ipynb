{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRhYvVtYGZqW"
      },
      "source": [
        "[![Open in Layer](https://development.layer.co/assets/badge.svg)](https://app.layer.ai/layer/Recommendation_System_and_Product_Categorisation_Project/) [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/layerai/examples/blob/main/recommendation-system/Recommendation_System.ipynb) [![Layer Examples Github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/layerai/examples/tree/main/recommendation-system)\n",
        "\n",
        "\n",
        "In this e-commerce example walkthrough, we will develop and build a Recommendation System on  Layer. We will use a public clickstream dataset for this example project. For more information about the dataset, you could check out its Kaggle page here: https://www.kaggle.com/datasets/tunguz/clickstream-data-for-online-shopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qNiWcT3GZqZ"
      },
      "source": [
        "# Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VozG3uzUGZqZ"
      },
      "outputs": [],
      "source": [
        "!pip install layer --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBWSNFMzXng4"
      },
      "outputs": [],
      "source": [
        "!rm -rf examples/recommendation-system\n",
        "!git clone https://github.com/layerai/examples/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WAR6cuxGZqb"
      },
      "source": [
        "# Getting Started with Layer\n",
        "\n",
        "Layer is an MLOps platform which advances ML pipelines with remote computation and tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SwMLbvRcck1g"
      },
      "outputs": [],
      "source": [
        "from layer.decorators import dataset, model,resources\n",
        "from layer.client import Dataset, Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5kT5VUQGZqc"
      },
      "source": [
        "## Login to Layer\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Let's login to Layer first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gghgHk5Wcqz7"
      },
      "outputs": [],
      "source": [
        "layer.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d2ECqoqGZqd"
      },
      "source": [
        "## Initialize Layer Project\n",
        "Now we are ready to init our project. Layer Project is basically an ML Repo hosted on Layer where you can store your datasets, models, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5J9gw3WkTkV",
        "outputId": "53731a34-f417-40d3-f317-6165ac6f6ec8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Project(name='Recommendation_System_and_Product_Categorisation_Project', raw_datasets=[], derived_datasets=[], models=[], path=PosixPath('.'), project_files_hash='', readme='', account=Account(id=UUID('add1b570-c8e7-4187-b747-1d01104893a9'), name='layer'), _id=UUID('3335ab76-e638-4bb5-92c3-caabd647e298'), functions=[])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "layer.init(\"Ecommerce_Recommendation_System\", pip_packages=[\"gensim\",\"sklearn\",\"pandas\",\"numpy\"], fabric=\"f-medium\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX0y19rxGZqe"
      },
      "source": [
        "Your project is ready. Find your project here:\n",
        "\n",
        "https://app.layer.ai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_26vqWxMGZqe"
      },
      "source": [
        "# **Data Transformation**\n",
        "\n",
        "We will create total of 4 Layer datasets in this project. Here is the list of those datasets and their little descriptions.\n",
        "\n",
        "*  **raw_session_based_clickstream_data:** This is basically identical to the source csv file which is a public Kaggle dataset: https://www.kaggle.com/datasets/tunguz/clickstream-data-for-online-shopping\n",
        "\n",
        "    It is just Layer Dataset definition of the same clickstream raw data.\n",
        "\n",
        "* **sequential_products:** This is a Layer dataset derived from the previous dataset which consists of sequences of products viewed in order per session. \n",
        "\n",
        "\n",
        "* **product_ids_and_vectors:** This is a Layer dataset which stores product vectors (embeddings) returned from Word2Vec algorithm.\n",
        "\n",
        "\n",
        "* **final_product_clusters:** This is a Layer dataset which stores assigned cluster numbers per product and other members of those clusters.\n",
        "\n",
        "Once you are done with building datasets, you can see them all here:\n",
        "\n",
        "https://app.layer.ai/layer/Recommendation_System_and_Product_Categorisation_Project/datasets/\n",
        "\n",
        "## **Model**\n",
        "\n",
        "We will be training a K-Means model from sklearn. We will fit our clustering model using product vectors that we have created previously. You can find all the model experiments and logged data here:\n",
        "\n",
        "Once you are done with training the model, you can see it here:\n",
        "\n",
        "https://app.layer.ai/layer/Recommendation_System_and_Product_Categorisation_Project/models/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3EUDpEOvInss"
      },
      "outputs": [],
      "source": [
        "@resources(\"examples/recommendation-system/e-shop clothing 2008.csv\")\n",
        "@dataset(\"raw_session_based_clickstream_data\")\n",
        "def raw_session_based_clickstream_data():\n",
        "  data = pd.read_csv(\"examples/recommendation-system/e-shop clothing 2008.csv\",delimiter = ';')\n",
        "\n",
        "  layer.log({\"Dataset Description\": \"Raw clickstream data of an e-commerce clothing company\"})\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oUD1gQxAKdQ4"
      },
      "outputs": [],
      "source": [
        "# remove consecutive duplicates from list\n",
        "def remove_consec_duplicates(raw_lst):\n",
        "  previous_value = None\n",
        "  new_lst = []\n",
        "\n",
        "  for elem in raw_lst:\n",
        "    if elem != previous_value:\n",
        "        new_lst.append(elem)\n",
        "        previous_value = elem\n",
        "        \n",
        "  return new_lst\n",
        "\n",
        "\n",
        "@dataset(\"sequential_products\",dependencies=[Dataset('raw_session_based_clickstream_data')])\n",
        "def generate_sequential_products():\n",
        "  # Load dataset\n",
        "  raw_clickstream = layer.get_dataset(\"raw_session_based_clickstream_data\").to_pandas()\n",
        "\n",
        "  # Rename columns\n",
        "  data = raw_clickstream.rename(columns={\"session ID\": \"session_id\", \"page 2 (clothing model)\": \"product_id\"})\n",
        "  # Remove sessions where only a single product is viewed\n",
        "  data = data.groupby(\"session_id\").filter(lambda x: len(x) > 1)\n",
        "  # Group product view sequences in order by session id\n",
        "  data = data.sort_values(\"order\").groupby(\"session_id\")[\"product_id\"].apply(list)\n",
        "  # Remove consecutive duplicate product views from the sequences genereated in the previous step\n",
        "  data = data.apply(remove_consec_duplicates)\n",
        "\n",
        "  #Convert series to data frame since you could only return dataframe in a Layer dataset decorator\n",
        "  data = data.to_frame().reset_index().rename(columns={\"product_id\": \"chronological_product_sequence\"})\n",
        "\n",
        "  layer.log({\"Dataset Description\": \"Session-based sequences of products in view order\"})\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqVnGUjtmicl"
      },
      "outputs": [],
      "source": [
        "@dataset(\"product_ids_and_vectors\", dependencies=[Dataset(\"sequential_products\")])\n",
        "def create_product_embeddings():\n",
        "  import gensim\n",
        "  from gensim.models import Word2Vec\n",
        "  # Fetch dataset from Layer\n",
        "  session_based_product_sequences = layer.get_dataset(\"sequential_products\").to_pandas()\n",
        "  \n",
        "  # Create Gensim CBOW model\n",
        "  session_product_sequences = session_based_product_sequences['chronological_product_sequence'].apply(list)\n",
        "  word2vec_model = gensim.models.Word2Vec(session_product_sequences, min_count = 1, size = 10, window = 5)\n",
        "  \n",
        "  # numpy.ndarrays of product vectors\n",
        "  product_vectors = word2vec_model.wv.vectors\n",
        "\n",
        "  productID_list = word2vec_model.wv.vocab.keys()\n",
        "  vector_list = word2vec_model.wv.vectors.tolist()\n",
        "  data_tuples = list(zip(productID_list,vector_list))\n",
        "  product_ids_and_vectors = pd.DataFrame(data_tuples, columns=['Product_ID','Vectors'])\n",
        "\n",
        "  layer.log({\"Dataset Description\": \"Product Vectors (Embeddings) generated by word2vec algorithm.\"})\n",
        "\n",
        "  return product_ids_and_vectors\n",
        "\n",
        "def plot_cluster_distribution(kmeans_model):\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  plt.hist(kmeans_model.labels_, rwidth=0.7)\n",
        "  plt.ylabel(\"Number of Products\")\n",
        "  plt.xlabel(\"Cluster No\")\n",
        "\n",
        "  # Layer logs the plot (figure)\n",
        "  fig = plt.gcf()\n",
        "  layer.log({\"Product Distribution over Clusters\": fig})\n",
        "\n",
        "  # clear all plots and figures from memory\n",
        "  plt.figure().clear()\n",
        "  plt.close()\n",
        "  plt.cla()\n",
        "  plt.clf()\n",
        "\n",
        "\n",
        "def plot_cluster_scatter(product_vectors):\n",
        "  import matplotlib.pyplot as plt\n",
        "  from sklearn.decomposition import PCA\n",
        "  from sklearn.cluster import KMeans\n",
        "\n",
        "  pca = PCA(n_components=2)\n",
        "  two_dimensions_vectors = pca.fit_transform(product_vectors)\n",
        "  \n",
        "  kmeans_model = KMeans(n_clusters=10, random_state=0).fit(two_dimensions_vectors)\n",
        "  label = kmeans_model.fit_predict(two_dimensions_vectors)\n",
        "\n",
        "  #Getting the Centroids\n",
        "  centroids = kmeans_model.cluster_centers_\n",
        "  u_labels = np.unique(kmeans_model.labels_)\n",
        "  \n",
        "  #plotting the results:\n",
        "  for i in u_labels:\n",
        "      plt.scatter(two_dimensions_vectors[label == i , 0] , two_dimensions_vectors[label == i , 1] , label = i)\n",
        "  plt.scatter(centroids[:,0] , centroids[:,1] , s = 80, color = 'k')\n",
        "  plt.legend(bbox_to_anchor =(1, 1))\n",
        "  \n",
        "\n",
        "  # Layer logs the plot (figure)\n",
        "  fig = plt.gcf()\n",
        "  layer.log({\"Clusters Formation Scatter Plot\": fig})\n",
        "\n",
        "  # clear all plots and figures from memory\n",
        "  plt.figure().clear()\n",
        "  plt.close()\n",
        "  plt.cla()\n",
        "  plt.clf()\n",
        "\n",
        "@model(\"clustering_model\",dependencies=[Dataset(\"product_ids_and_vectors\")])\n",
        "def fit_kmeans():\n",
        "  import gensim\n",
        "  from gensim.models import Word2Vec\n",
        "  from sklearn.cluster import KMeans\n",
        "  import matplotlib.pyplot as plt\n",
        "  \n",
        "  # Get product vectors from Word2Vec\n",
        "  product_ids_and_vectors = layer.get_dataset(\"product_ids_and_vectors\").to_pandas()\n",
        "  array_product_vectors = np.array(product_ids_and_vectors[\"Vectors\"].values.tolist())\n",
        "\n",
        "  # Fit K-Means algorithm on those embeddings\n",
        "  kmeans_model = KMeans(n_clusters=10, random_state=0).fit(array_product_vectors)\n",
        "\n",
        "  # Cluster Distribution Plot\n",
        "  plot_cluster_distribution(kmeans_model)\n",
        "\n",
        "  # Cluster Scatter Plot\n",
        "  plot_cluster_scatter(array_product_vectors)\n",
        "  \n",
        "  return kmeans_model\n",
        "\n",
        "@dataset(\"final_product_clusters\", dependencies=[Model(\"clustering_model\"), Dataset(\"product_ids_and_vectors\")])\n",
        "def save_final_product_clusters():\n",
        "  model = layer.get_model(\"clustering_model\").get_train()\n",
        "  \n",
        "  product_ids_and_vectors = layer.get_dataset(\"product_ids_and_vectors\").to_pandas()\n",
        "  array_product_vectors = np.array(product_ids_and_vectors[\"Vectors\"].values.tolist())\n",
        "\n",
        "  assigned_cluster_no = model.fit_predict(array_product_vectors).tolist()\n",
        "\n",
        "  df[\"Cluster_No\"] = assigned_cluster_no\n",
        "  cluster_members_df = df[[\"Product_ID\",\"Cluster_No\"]].groupby(\"Cluster_No\")['Product_ID'].apply(list).to_frame().reset_index().rename(columns={'Product_ID': 'Cluster_Member_List'})\n",
        "\n",
        "  layer.log({\"Dataset Description\": \"Product Clusters Members List\"})\n",
        "\n",
        "  return cluster_members_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFCHzvX3GZqg"
      },
      "source": [
        "# **Run your project on Layer in local mode**\n",
        "\n",
        "To run your project locally on Layer, just call your functions in the correct order and Layer will log and save all the data you asked for automatically.\n",
        "\n",
        "If you would like to run it in remote mode, put all you functions into the layer.run(), then you are good to go! It will run these functions in its correct order on Layer Infra remotely with respect to dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cenP3eBCmS92"
      },
      "outputs": [],
      "source": [
        "## LAYER LOCAL MODE\n",
        "raw_session_based_clickstream_data()\n",
        "generate_sequential_products()\n",
        "create_product_embeddings()\n",
        "fit_kmeans()\n",
        "save_final_product_clusters()\n",
        "\n",
        "\n",
        "## LAYER REMOTE MODE\n",
        "# layer.run([raw_session_based_clickstream_data,\n",
        "#            generate_sequential_products,\n",
        "#            create_product_embeddings, \n",
        "#            fit_kmeans,\n",
        "#            save_final_product_clusters],debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqm345-PGZqh"
      },
      "source": [
        "# **Let's produce some recommendations for a given product!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAfyseR8bnsj",
        "outputId": "29f475d4-06d4-4e5c-80ad-2b3d9cbceac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5 Similar Product Recommendations for A13:  ['C17', 'P60', 'C44', 'P56', 'A6']\n"
          ]
        }
      ],
      "source": [
        "# Fetch the K-Means Model from Layer\n",
        "kmeans_model = layer.get_model(\"clustering_model\").get_train()\n",
        "\n",
        "# Fetch product vectors (embeddings) dataset from Layer\n",
        "product_ids_and_vectors = layer.get_dataset(\"product_ids_and_vectors\").to_pandas()\n",
        "\n",
        "# Product ID to generate recommendations for - You could try different product IDs in the data such as A16, C17, P12 etc.\n",
        "product_id = \"A13\"\n",
        "\n",
        "# Get Vector (Embedding) array of the given product\n",
        "vector_array = np.array(product_ids_and_vectors[product_ids_and_vectors[\"Product_ID\"]==product_id][\"Vectors\"].tolist())\n",
        "\n",
        "# Get cluster number for the given product assigned by the model\n",
        "cluster_no = model.predict(vector_array)[0]\n",
        "\n",
        "# Fetch final clusters members list dataset from Layer\n",
        "final_product_clusters = layer.get_dataset(\"final_product_clusters\").to_pandas()\n",
        "\n",
        "# Get members list of the cluster that the given product is assigned to \n",
        "cluster_members_list = final_product_clusters[final_product_clusters['Cluster_No']==cluster_no]['Cluster_Member_List'].iloc[0].tolist()\n",
        "\n",
        "# Randomly select 5 product recommendations from the cluster members excluding the given product\n",
        "from random import sample\n",
        "cluster_members_list.remove(product_id)\n",
        "five_product_recommendations = sample(cluster_members_list, 5)\n",
        "\n",
        "print(\"5 Similar Product Recommendations for {}: \".format(product_id),five_product_recommendations)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Recommendation System and Product Categorisation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
