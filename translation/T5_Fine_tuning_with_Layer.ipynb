{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5 Fine tuning with Layer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning T5 with Layer\n",
        "\n",
        "[![Open In Layer](https://uploads-ssl.webflow.com/6090530ad3595f001f4f9084/623dc0c25d2cf70aa28210bc_layer_badge.svg)](https://development.layer.co/layer/t5-fine-tuning-with-layer?tab=assets) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1fppz3WBPnX-XUO4ucRe1cv6CUXNCrBAg?usp=sharing)\n",
        "\n",
        "A T5 is an encoder-decoder model. It converts all NLP problems like language translation, summarization, text generation, question-answering, to a text-to-text task.\n",
        "\n",
        "We are going to fine tune a pretrained T5 Model from ðŸ¤— and train it to translate English to SQL."
      ],
      "metadata": {
        "id": "hbl_5GrpV_Qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Requirements"
      ],
      "metadata": {
        "id": "UTOTXBFNKqhb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkjmRTy_VQ72"
      },
      "outputs": [],
      "source": [
        "!pip install layer-sdk --upgrade -q\n",
        "!pip install sentencepiece -q\n",
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from layer.decorators.assertions import assert_true, assert_valid_values, assert_not_null, assert_unique\n",
        "from layer.decorators import dataset, model,resources, pip_requirements,fabric\n",
        "from layer.client import Dataset\n",
        "import layer\n",
        "import torch\n",
        "import random\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "UwlK1RxFKIWc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started with Layer\n",
        "\n",
        "Layer is an MLOps platform which advances ML pipelines with remote computation and tracking."
      ],
      "metadata": {
        "id": "-ZPKqz1aHjDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to Layer\n",
        "\n",
        "Let's login to Layer first."
      ],
      "metadata": {
        "id": "X4eVYhZ8K-Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer.login(\"https://development.layer.co\")"
      ],
      "metadata": {
        "id": "AW6owUH0VWe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Layer Project\n",
        "Now we are ready to init our project. Layer Project is basically an ML Repo hosted on Layer where you can store your datasets, models, metrics"
      ],
      "metadata": {
        "id": "Vgj3JHo-JeNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer.init(\"t5-fine-tuning-with-layer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezPdWXBXVlBP",
        "outputId": "90d87f6e-e9c3-4e7e-dd06-adc6eab57a52"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Project(name='t5-fine-tuning-with-layer', raw_datasets=[], derived_datasets=[], featuresets=[], models=[], path=PosixPath('.'), project_files_hash='', readme='', organization=Organization(id=UUID('d7325da3-0646-4fa6-855d-8d19eece8b79'), name='layer'), _id=UUID('4426cf2d-bc6d-487f-902a-1b106e04da58'), functions=[])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your project is ready. Find your project here:\n",
        "\n",
        "https://development.layer.co"
      ],
      "metadata": {
        "id": "Xos7VwStJq_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Generation\n",
        "Unlike language to language translation datasets, we can build custom English to SQL translation pairs programmatically with the help of some templates."
      ],
      "metadata": {
        "id": "63lSAPAiWeyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "templates = [\n",
        "              [\"[prop1] of [nns]\",\"SELECT [prop1] FROM [nns]\"],\n",
        "              [\"[agg] [prop1] for each [breakdown]\",\"SELECT [agg]([prop1]) , [breakdown] FROM [prop1] GROUP BY [breakdown]\"],\n",
        "              [\"[prop1] of [nns] by [breakdown]\",\"SELECT [prop1] , [breakdown] FROM [nns] GROUP BY [breakdown]\"],\n",
        "              [\"[prop1] of [nns] in [location] by [breakdown]\",\"SELECT [prop1] , [breakdown] FROM [nns] WHERE location = '[location]' GROUP BY [breakdown]\"],\n",
        "              [\"[nns] having [prop1] between [number1] and [number2]\",\"SELECT name FROM [nns] WHERE [prop1] > [number1] and [prop1] < [number2]\"],\n",
        "              [\"[prop] by [breakdown]\",\"SELECT name , [breakdown] FROM [prop] GROUP BY [breakdown]\"],\n",
        "              [\"[agg] of [prop1] of [nn]\",\"SELECT [agg]([prop1]) FROM [nn]\"],\n",
        "              [\"[prop1] of [nns] before [year]\",\"SELECT [prop1] FROM [nns] WHERE date < [year]\"],\n",
        "              [\"[prop1] of [nns] after [year] in [location]\",\"SELECT [prop1] FROM [nns] WHERE date > [year] AND location='[location]'\"],\n",
        "              [\"[nns] [verb] after [year] in [location]\",\"SELECT name FROM [nns] WHERE location = '[location]' AND date > [year]\"],\n",
        "              [\"[nns] having [prop1] between [number1] and [number2] by [breakdown]\",\"SELECT name , [breakdown] FROM [nns] WHERE [prop1] < [number1] AND [prop1] > [number2] GROUP BY [breakdown]\"],\n",
        "              [\"[nns] with a [prop1] of maximum [number1] by their [breakdown]\",\"SELECT name , [breakdown] FROM [nns] WHERE [prop1] <= [number1] GROUP BY [breakdown]\"],\n",
        "              [\"[prop1] and [prop2] of [nns] since [year]\",\"SELECT [prop1] , [prop2] FROM [nns] WHERE date > [year]\"],\n",
        "              [\"[nns] which have both [prop1] and [prop2]\",\"SELECT name FROM [nns] WHERE [prop1] IS true AND [prop2] IS true\"],\n",
        "              [\"Top [number1] [nns] by [prop1]\",\"SELECT name FROM [nns] ORDER BY [prop1] DESC LIMIT [number1]\"]\n",
        "]\n",
        "template = random.choice(templates)\n",
        "print(\"Sample Query Template  :\", template[0])\n",
        "print(\"SQL Translation        :\", template[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ablj8OlgWlC2",
        "outputId": "33a030ec-d0a7-400a-ffcc-d17feb2a9a38"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Query Template  : Top [number1] [nns] by [prop1]\n",
            "SQL Translation        : SELECT name FROM [nns] ORDER BY [prop1] DESC LIMIT [number1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "objects = [\"countries\",\"wines\",\"wineries\",\"tasters\", \"provinces\",\"grapes\",\"cities\",\"bottles\",\"deliveries\"]\n",
        "object_single = [\"country\",\"wine\",\"winery\",\"taster\", \"province\",\"grape\",\"city\",\"bottle\", \"delivery\"]\n",
        "properties = [\"points\",\"price\",\"taste\",\"title\",\"texture\",\"age\",\"duration\",\"acidity\",\"flavor\",\"level\"]\n",
        "aggs = [[\"average\",\"avg\"], [\"total\",\"sum\"],[\"count\",\"count\"], [\"minimum\",\"min\"], [\"maximum\",\"max\"]]\n",
        "breakdowns = [\"quality\",\"price\",\"province\",\"country\",\"point\", \"variety\",\"flavor\",\"age\"]\n",
        "locations = [\"Italy\",\"US\",\"Portugal\",\"Spain\",\"Chile\",\"Turkey\",\"Canada\"]\n",
        "verbs = [\"produced\",\"bottled\"]\n",
        "\n",
        "regex = r\"\\[([a-z0-9]*)\\]\"\n",
        "number_of_samples = 2500\n",
        "\n",
        "@dataset(\"english_sql_translations\")\n",
        "def build_dataset():\n",
        "    rows = []\n",
        "    for index in range(0,number_of_samples):\n",
        "        template = random.choice(templates)\n",
        "        nl = template[0]\n",
        "        sql = template[1]\n",
        "\n",
        "        matches = re.finditer(regex, nl, re.MULTILINE)\n",
        "\n",
        "        for matchNum, match in enumerate(matches, start=1):\n",
        "            key = match.group()\n",
        "            prop = None\n",
        "            prop_sql = None\n",
        "            if key.startswith(\"[prop\"):\n",
        "                prop = random.choice(properties)\n",
        "                prop_sql = prop.replace(\" \",\"_\").lower()\n",
        "            if key in [\"[nns]\"]:\n",
        "                prop = random.choice(objects)\n",
        "                prop_sql = prop\n",
        "            if key in [\"[nn]\"]:\n",
        "                prop = random.choice(object_single)\n",
        "                prop_sql = prop.replace(\" \",\"_\").lower()\n",
        "            if key == \"[breakdown]\":\n",
        "                prop = random.choice(breakdowns)\n",
        "                prop_sql = prop.replace(\" \",\"_\").lower()\n",
        "            if key == \"[verb]\":\n",
        "                prop = random.choice(verbs)\n",
        "                prop_sql = prop.replace(\" \",\"_\").lower()\n",
        "            if key == \"[agg]\":\n",
        "                aggregation = random.choice(aggs)\n",
        "                prop = aggregation[0]\n",
        "                prop_sql = aggregation[1]\n",
        "            if key == \"[location]\":\n",
        "                prop = random.choice(locations)\n",
        "                prop_sql = prop\n",
        "            if key.startswith(\"[number\"):\n",
        "                prop = str(random.randint(1,1000))\n",
        "                prop_sql = prop\n",
        "            if key.startswith(\"[year\"):\n",
        "                prop = str(random.randint(1950,2022))\n",
        "                prop_sql = prop\n",
        "            \n",
        "\n",
        "            if prop is not None:\n",
        "                nl = nl.replace(key,prop)\n",
        "                sql = sql.replace(key,prop_sql)\n",
        "        \n",
        "        prefix = random.randint(1,20)\n",
        "        if prefix == 1:\n",
        "            nl = \"Show me \"+nl\n",
        "        elif prefix == 2:\n",
        "            nl = \"List \"+nl\n",
        "        elif prefix == 3:\n",
        "            nl = \"List of \"+nl\n",
        "        elif prefix == 4:\n",
        "            nl = \"Find \"+nl\n",
        "        rows.append([nl,sql])\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\"query\", \"sql\"])\n",
        "    return df"
      ],
      "metadata": {
        "id": "zbZ8S8m5YXNx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register dataset to Layer\n",
        "\n",
        "In the above cell, we have used a special decorator called `@dataset` which tells Layer that our function creates dataset. Now we are going to pass this function to Layer to be run on Layer infra and register the built dataset under our project."
      ],
      "metadata": {
        "id": "7BEjcCk_MJ95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer.run([build_dataset])"
      ],
      "metadata": {
        "id": "OdhJXS-YLmtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Data Loader"
      ],
      "metadata": {
        "id": "9LarAgyNLUPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnglishToSQLDataSet(Dataset):\n",
        "\n",
        "  def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataframe\n",
        "    self.source_len = source_len\n",
        "    self.summ_len = target_len\n",
        "    self.target_text = self.data[target_text]\n",
        "    self.source_text = self.data[source_text]\n",
        "\n",
        "    self.data[\"query\"] = \"translate English to SQL: \"+self.data[\"query\"]\n",
        "    self.data[\"sql\"] = \"<pad>\" + self.data[\"sql\"] + \"</s>\"\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.target_text)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    source_text = str(self.source_text[index])\n",
        "    target_text = str(self.target_text[index])\n",
        "\n",
        "    source_text = ' '.join(source_text.split())\n",
        "    target_text = ' '.join(target_text.split())\n",
        "\n",
        "    source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "    target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "\n",
        "    source_ids = source['input_ids'].squeeze()\n",
        "    source_mask = source['attention_mask'].squeeze()\n",
        "    target_ids = target['input_ids'].squeeze()\n",
        "    target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "    return {\n",
        "        'source_ids': source_ids.to(dtype=torch.long),\n",
        "        'source_mask': source_mask.to(dtype=torch.long),\n",
        "        'target_ids': target_ids.to(dtype=torch.long),\n",
        "        'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "    }"
      ],
      "metadata": {
        "id": "r51__wGKjWhq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tune T5\n",
        "\n",
        "Our dataset is ready and registered to Layer. Now we are going to develop the fine tuning logic, decorate the function with `@model` and pass it to Layer so that it can be run on Layer infra and registered under our project"
      ],
      "metadata": {
        "id": "KhDn_iW5NLyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "  import torch\n",
        "  \n",
        "  model.train()\n",
        "  for _,data in enumerate(loader, 0):\n",
        "    y = data['target_ids'].to(device, dtype = torch.long)\n",
        "    y_ids = y[:, :-1].contiguous()\n",
        "    lm_labels = y[:, 1:].clone().detach()\n",
        "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "    ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "    mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "    loss = outputs[0]\n",
        "\n",
        "    if _%50==0:\n",
        "      print(str(epoch), str(_), str(loss), flush=True)\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "yUxkIObxoh8j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use 3 seperate Layer decorators:\n",
        "- [`@model`](https://docs.development.layer.co/docs/sdk-library/model-decorator): Tells Layer that this function trains an ML model\n",
        "- [`@fabric`](https://docs.development.layer.co/docs/sdk-library/fabric-decorator): Tells Layer the computation resources (cpu, gpu etc.) needed to train the model. Here is a list of the [available fabrics](https://docs.development.layer.co/docs/reference/fabrics) you can use.\n",
        "- [`@pip_requirements`](https://docs.development.layer.co/docs/sdk-library/pip-requirements-decorator): Tells the pypi libraries needed to train the model."
      ],
      "metadata": {
        "id": "pqA2YcZ8NdKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@model(\"t5-tokenizer\")\n",
        "@fabric(\"f-medium\")\n",
        "@pip_requirements(packages=[\"torch\",\"transformers\",\"sentencepiece\"])\n",
        "def build_tokenizer():\n",
        "  from transformers import T5Tokenizer\n",
        "  tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        "  return tokenizer\n",
        "\n",
        "@model(\"t5-english-to-sql\")\n",
        "@fabric(\"f-medium\")\n",
        "@pip_requirements(packages=[\"torch\",\"transformers\",\"sentencepiece\"])\n",
        "def build_model():\n",
        "  from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "  from torch import cuda\n",
        "  from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "  import torch.nn.functional as F\n",
        "  import torch\n",
        "\n",
        "  # # Set random seeds and deterministic pytorch for reproducibility\n",
        "  torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
        "  np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  # Setting up the device for GPU usage\n",
        "  device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "  print(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "  # Load tokenizer\n",
        "  tokenizer = layer.get_model(\"t5-tokenizer\").get_train()\n",
        "  \n",
        "  # Load pretrained model from Layer\n",
        "  model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "  model.to(device)\n",
        "\n",
        "  print(f\"[Data]: Reading data...\\n\")\n",
        "  \n",
        "  dataframe = layer.get_dataset(\"english_sql_translations\").to_pandas()\n",
        "  source_text = \"query\"\n",
        "  target_text = \"sql\"\n",
        "\n",
        "  # Importing the raw dataset\n",
        "  dataframe = dataframe[[source_text,target_text]]\n",
        "\n",
        "  # Creation of Dataset and Dataloader\n",
        "  # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
        "  train_size = 0.8\n",
        "  train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
        "  val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "  train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "  print(f\"FULL Dataset: {dataframe.shape}\")\n",
        "  print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "  print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
        "\n",
        "  # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "  training_set = EnglishToSQLDataSet(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
        "  val_set = EnglishToSQLDataSet(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
        "\n",
        "  # Defining the parameters for creation of dataloaders\n",
        "  train_params = {\n",
        "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "      'shuffle': True,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "\n",
        "  val_params = {\n",
        "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
        "      'shuffle': False,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "\n",
        "  # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "  training_loader = DataLoader(training_set, **train_params)\n",
        "  val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "\n",
        "  # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "  optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
        "\n",
        "\n",
        "  # Training loop\n",
        "  print(f'[Initiating Fine Tuning]...\\n')\n",
        "\n",
        "  for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "      train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "  print(f\"[Saving Model]...\\n\")\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "NQYIvpAFoo2-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_params={\n",
        "    \"MODEL\":\"t5-small\",            \n",
        "    \"TRAIN_BATCH_SIZE\":8,          \n",
        "    \"VALID_BATCH_SIZE\":8,          \n",
        "    \"TRAIN_EPOCHS\":3,              \n",
        "    \"VAL_EPOCHS\":3,                \n",
        "    \"LEARNING_RATE\":1e-4,          \n",
        "    \"MAX_SOURCE_TEXT_LENGTH\":75,   \n",
        "    \"MAX_TARGET_TEXT_LENGTH\":75,\n",
        "    \"SEED\": 33\n",
        "}\n",
        "\n",
        "# # You can train your model locally by just calling the function to debug your code.\n",
        "# build_tokenizer()\n",
        "# build_model()\n",
        "\n",
        "# # Once you are ready, you can push your model training function to Layer to be trained.\n",
        "layer.run([build_tokenizer, build_model], debug=True)"
      ],
      "metadata": {
        "id": "fFWl17gYpOA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "47VEanm48tcs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}